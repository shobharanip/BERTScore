
⸻

1. Future improvements for latency (in models like MADLAD/BERTScore)

Latency = how long it takes for a model to give results.

Future improvements may include:
	•	Model quantization: Make models smaller/faster by using lower precision (e.g., INT8).
	•	Distillation: Train smaller models from big ones that work nearly as well (e.g., TinyBERT).
	•	Batch processing and caching: Faster execution for repeated tasks.
	•	On-device inference: Run models on edge devices to avoid network delay.
	•	Better hardware use: Use optimized GPU/TPU inference with libraries like ONNX or TensorRT.

⸻

2. Why rescale with baseline in BERTScore?

BERTScore compares similarity between predicted and reference sentences using BERT embeddings.

Rescaling with baseline means:
	•	You adjust raw similarity scores using precomputed baseline values (based on random sentence pairs).
	•	This helps make scores more comparable across languages or different sentence lengths.
	•	Without rescaling, scores might be misleading due to BERT’s natural biases.

Simple analogy: Like adjusting your test score based on the difficulty level of the exam.

⸻

3. What is number of layer 17 in BERTScore?

When using BERT, it has multiple layers (usually 12 for BERT-base, 24 for BERT-large).
	•	Layer 17 is one of the deeper layers of a BERT-large or RoBERTa-large model.
	•	BERTScore often uses embeddings from a specific layer to calculate sentence similarity.
	•	Layers 8–17 are common choices because deeper layers better understand meaning, but too deep can overfit.

⸻

4. Why lang=en in BERTScore?

This tells BERTScore to:
	•	Use a language-specific model (like English RoBERTa for lang=en).
	•	Apply tokenization and processing that’s optimized for English.
	•	Helps get more accurate similarity scores than using a multilingual model.

⸻

5. Why use RoBERTa as the language model in BERTScore?

RoBERTa is a stronger version of BERT:
	•	Trained longer and with more data.
	•	Better performance in many tasks.
	•	Used in BERTScore because it gives richer embeddings, improving score quality.

⸻

6. Future plans for models like BERTScore?

Here are some likely directions:
	•	Multilingual improvements: Better support for low-resource languages.
	•	Faster inference: Using smaller models or ONNX for real-time scoring.
	•	Robustness: Make scores more stable across different languages and sentence lengths.
	•	Task-specific tuning: Adapt BERTScore to work better on translation, summarization, etc.
	•	Explainability: Show which words contribute most to the score.