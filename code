Baseline (no warmup, no smoothing)
it drops “are we” because the first big updates scramble small function words.




2) Add warmup_steps=500
	•	In plain terms:
You don’t shove the bike forward at full speed—you steady the learner with small pushes first.
	•	Effect:
The model’s first 500 steps use a very low LR, so it learns those tiny helper words before speeding up.

now “are we” is back, though the main verb still isn’t quite right.

⸻

3) Also add label_smoothing_factor=0.1
	•	In plain terms:
Instead of saying “I want exactly this word,” you say “I want mostly this word, but maybe a tiny bit of other words too.”
	•	Effect:
The model stops over‑committing to the exact training phrasing, so it chooses the correct verb form and natural word order.
	•	Final output:


perfect match, with both the auxiliary (“are we”) and the gerund (“requesting”) correctly placed.

⸻

Bottom line:
	•	Warmup brings back tiny but crucial words (“are we”).
	•	Smoothing polishes the verb form and phrasing (“requesting”).

Together they turn

“Why request this information?”
into
“Why are we requesting this information?”



With our new warmup_steps=500 it’ll ramp the learning‑rate slowly, so early on it won’t overshoot and drop words.
	•	And label_smoothing_factor=0.1 keeps it from becoming over‑confident on any single token.


 args = Seq2SeqTrainingArguments(
     output_dir="…",
     do_train=True,
     eval_steps=100,
     learning_rate=1.2453e-6,
     per_device_train_batch_size=batch_size,
     per_device_eval_batch_size=batch_size,
     weight_decay=0.00227,
     save_total_limit=1,
     num_train_epochs=14,
     predict_with_generate=True,
+    warmup_steps=500,              # ← slowly ramp up the LR over 500 steps
+    label_smoothing_factor=0.1,    # ← smooth the target distribution by 0.1
 )

import os
import warnings
import pandas as pd
import numpy as np
import torch
from datasets import Dataset
from nltk.translate.bleu_score import sentence_bleu
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback,
    logging,
)
from accelerate import Accelerator, DataLoaderConfiguration
from peft import LoraConfig, get_peft_model

# ─── Setup ──────────────────────────────────────────────────────────────────────
warnings.filterwarnings("ignore")
logging.set_verbosity_error()
torch.manual_seed(0)
os.environ["CUDA_VISIBLE_DEVICES"] = "1,2"
print("GPUs available:", torch.cuda.device_count())

# ─── Hyperparameters ────────────────────────────────────────────────────────────
checkpoint                   = "/appdata/cortex/dev1/aptaiModels/madlad400-3b-mt"
output_dir                   = "/appdata/cortex/dev1/shob/Refined_datasets"
max_length                   = 128
batch_size                   = 8
gradient_accumulation_steps  = 4
learning_rate                = 1.245e-06
weight_decay                 = 0.00227
num_train_epochs             = 14
eval_steps                   = 100
warmup_ratio                 = 0.1
label_smoothing_factor       = 0.1
lr_scheduler_type            = "cosine"
bleu_workers                 = 25

# ─── Load Model & Tokenizer ─────────────────────────────────────────────────────
tokenizer  = T5Tokenizer.from_pretrained(checkpoint)
base_model = T5ForConditionalGeneration.from_pretrained(
    checkpoint,
    low_cpu_mem_usage=True,
    offload_state_dict=True
)

# ─── Data Collator ───────────────────────────────────────────────────────────────
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=base_model,
    padding=True,
    pad_to_multiple_of=8
)

# ─── Load & Prepare Datasets ────────────────────────────────────────────────────
def load_lower_df(path):
    df = pd.read_excel(path)
    return df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

train_df = load_lower_df(f"{output_dir}/refined_train_df.xlsx")
val_df   = load_lower_df(f"{output_dir}/validation_df.xlsx")

train_ds = Dataset.from_pandas(train_df)
eval_ds  = Dataset.from_pandas(val_df)

def preprocess_function(examples):
    inputs  = examples["es"]
    targets = examples["en"]
    model_inputs = tokenizer(
        inputs,
        text_target=targets,
        max_length=max_length,
        truncation=True
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=max_length,
            truncation=True
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_ds.map(
    preprocess_function,
    batched=True,
    remove_columns=["es","en"]
)
tokenized_eval = eval_ds.map(
    preprocess_function,
    batched=True,
    remove_columns=["es","en"]
)

# ─── Attach LoRA Adapters ───────────────────────────────────────────────────────
lora_config = LoraConfig(
    r               = 32,
    lora_alpha      = 64,
    target_modules  = ["q_proj","v_proj","k_proj"],
    lora_dropout    = 0.2,
    bias            = "all",        # fine‑tune all bias terms
    fan_in_fan_out  = True          # match model’s internal weight layout
)
model = get_peft_model(base_model, lora_config)
model.gradient_checkpointing_enable()

# ─── BLEU Metric Functions ─────────────────────────────────────────────────────
def postprocess_text(preds, labels):
    preds  = [p.strip() for p in preds]
    labels = [[l.strip()] for l in labels]
    return preds, labels

def get_transMetrics_single(t, r):
    bleu = sentence_bleu([r.split()], t.split(), weights=(0.75,0.25,0,0))
    return {"BLEU": round(bleu,2)}

def get_transMetrics_batch(translation, reference, workers=bleu_workers):
    flat_ref = (
        [item for sublist in reference for item in sublist]
        if isinstance(reference[0], list) else reference
    )
    with ThreadPoolExecutor(max_workers=workers) as exe:
        futures = [exe.submit(get_transMetrics_single, t, r)
                   for t, r in zip(translation, flat_ref)]
        results = [f.result() for f in as_completed(futures)]
    return pd.DataFrame(results).mean().round(2).to_dict()

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
    return get_transMetrics_batch(decoded_preds, decoded_labels)

# ─── Accelerator & Dataloader Config ──────────────────────────────────────────
dataloader_config = DataLoaderConfiguration(dispatch_batches=False, split_batches=True)
accelerator = Accelerator(
    device_placement            = True,
    gradient_accumulation_steps = gradient_accumulation_steps,
    mixed_precision             = "fp16",
    dataloader_config           = dataloader_config
)

# ─── TrainingArguments & Trainer ───────────────────────────────────────────────
training_args = Seq2SeqTrainingArguments(
    output_dir                  = output_dir,
    do_train                    = True,
    evaluation_strategy         = "steps",
    eval_steps                  = eval_steps,
    save_strategy               = "steps",
    save_steps                  = eval_steps,
    load_best_model_at_end      = True,
    metric_for_best_model       = "BLEU",
    greater_is_better           = True,
    learning_rate               = learning_rate,
    per_device_train_batch_size = batch_size,
    per_device_eval_batch_size  = batch_size,
    weight_decay                = weight_decay,
    warmup_ratio                = warmup_ratio,
    lr_scheduler_type           = lr_scheduler_type,
    label_smoothing_factor      = label_smoothing_factor,
    num_train_epochs            = num_train_epochs,
    predict_with_generate       = True,
    logging_strategy            = "steps",
    logging_steps               = 50,
    save_total_limit            = 2,
)

trainer = Seq2SeqTrainer(
    model           = model,
    args            = training_args,
    train_dataset   = tokenized_train,
    eval_dataset    = tokenized_eval,
    data_collator   = data_collator,
    tokenizer       = tokenizer,
    compute_metrics = compute_metrics,
    callbacks       = [EarlyStoppingCallback(early_stopping_patience=3)],
)

# ─── Prepare & Start Training ──────────────────────────────────────────────────
trainer = accelerator.prepare(trainer)
result  = trainer.train()
print(result)