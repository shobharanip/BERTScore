import time
import pandas as pd
from sacrebleu import sentence_bleu, sentence_chrf
from sacrebleu.metrics import CHRF
from typing import Union, List, Optional, Dict
from pathlib import Path
import warnings, os

class TranslationEvaluator:
    """
    - Load translations (CSV or Excel)
    - Compute sentence‑level metrics: BLEU, ChrF, ChrF++
    - Compute model‑level metrics as the average of sentence scores
    - Export both to an Excel workbook
    """
    def __init__(self, output_file: Union[str, Path]="output_data.xlsx"):
        os.environ["TRANSFORMERS_CACHE"] = r"C:/Users/.../roberta-large"
        warnings.simplefilter("ignore")
        self._data             = None
        self._detailed_results = None
        self._model_metrics    = None
        self._output_file      = Path(output_file)
        self._chrff            = CHRF(word_order=2)

    def load_data(self, file_path: Union[str, Path]) -> None:
        file_path = Path(file_path)
        if file_path.suffix == ".csv":
            self._data = pd.read_csv(file_path)
        elif file_path.suffix in (".xls", ".xlsx"):
            self._data = pd.read_excel(file_path)
        else:
            raise ValueError(f"Unsupported file type: {file_path.suffix}")

    def evaluate(
        self,
        prediction_cols: List[str],
        reference_col:   str               = "en",
        metrics:         List[str]         = ["BLEU","ChrF","ChrF++"],
        keep_cols:       Optional[List[str]] = None
    ) -> None:
        if self._data is None:
            raise ValueError("No data loaded. Call load_data() first.")

        # 1) filter out any prediction_cols not actually in your DataFrame
        available = [c for c in prediction_cols if c in self._data.columns]
        missing   = set(prediction_cols) - set(available)
        if missing:
            print(f"⚠️  Warning: these prediction columns were not found and will be skipped: {missing}")
        prediction_cols = available
        if not prediction_cols:
            raise ValueError("No valid prediction columns left after filtering.")

        # 2) build keep_cols list
        keep = keep_cols or []
        if reference_col not in keep:
            keep.insert(0, reference_col)

        # 3) validate that ref & keep are in the data
        req = {reference_col} | set(keep)
        not_found = [c for c in req if c not in self._data.columns]
        if not_found:
            raise ValueError(f"Missing required columns: {not_found}")

        # 4) compute everything
        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep)
        self._compute_model_metrics   (prediction_cols, metrics)
        self._generate_report()

    def _compute_detailed_metrics(
        self,
        reference_col:   str,
        prediction_cols: List[str],
        metrics:         List[str],
        keep_cols:       List[str]
    ):
        df = self._data[keep_cols].copy()
        for pred in prediction_cols:
            if "BLEU" in metrics:
                df[f"{pred} BLEU"] = df.apply(
                    lambda r: sentence_bleu(r[pred], references=[r[reference_col]]).score
                              if pd.notna(r[pred]) and r[pred].strip()
                              else 0.0,
                    axis=1
                )
            if "ChrF" in metrics:
                df[f"{pred} ChrF"] = df.apply(
                    lambda r: sentence_chrf(r[pred], references=[r[reference_col]]).score
                              if pd.notna(r[pred]) and r[pred].strip()
                              else 0.0,
                    axis=1
                )
            if "ChrF++" in metrics:
                df[f"{pred} ChrF++"] = df.apply(
                    lambda r: self._chrff.sentence_score(r[pred], references=[r[reference_col]]).score
                              if pd.notna(r[pred]) and r[pred].strip()
                              else 0.0,
                    axis=1
                )

        # reorder: keep_cols, then each pred's metrics
        cols = keep_cols.copy()
        for p in prediction_cols:
            for m in metrics:
                cols.append(f"{p} {m}")
        self._detailed_results = df[cols]

    def _compute_model_metrics(
        self,
        prediction_cols: List[str],
        metrics:         List[str]
    ):
        results = []
        for pred in prediction_cols:
            row = {"Model": pred}
            for m in metrics:
                col = f"{pred} {m}"
                if col in self._detailed_results:
                    row[m] = self._detailed_results[col].mean()
            results.append(row)
        self._model_metrics = pd.DataFrame(results)

    def _generate_report(self):
        out = self._output_file
        out.parent.mkdir(exist_ok=True, parents=True)
        with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
            self._detailed_results.to_excel(writer, sheet_name="Detailed metrics", index=False)
            self._model_metrics   .to_excel(writer, sheet_name="Model metrics",    index=False)

            wb    = writer.book
            sheet = writer.sheets["Model metrics"]
            r, c  = self._model_metrics.shape
            headers = [{"header":x} for x in self._model_metrics.columns]
            sheet.add_table(0, 0, r, c-1, {
                "columns": headers,
                "style":   "Table Style Medium 9",
                "name":    "ModelMetrics"
            })
            for idx, colname in enumerate(self._model_metrics.columns):
                w = max(self._model_metrics[colname].astype(str).map(len).max(), len(colname)) + 2
                sheet.set_column(idx, idx, w)

        print(f"Report saved to {out.resolve()}")

    def get_detailed_results(self) -> pd.DataFrame:
        return self._detailed_results.copy()

    def get_model_metrics(self) -> pd.DataFrame:
        return self._model_metrics.copy()