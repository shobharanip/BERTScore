wanted to share a quick update on the recent fine-tuning experiments I’ve been working on.

As part of the optimization process, I applied regularization techniques during model fine-tuning to enhance generalization and mitigate overfitting. These adjustments have led to noticeable improvements in key evaluation metrics — specifically, the BLEU and chrF scores showed significant gains compared to previous iterations.

I’ll continue refining the model and will follow up with a detailed summary of the results, including visualizations and comparisons, once all runs are complete.

Please let me know if you’d like to review the interim results or discuss any next steps.


Cumulative Transcript (truncated)
Summary_Transcript_CPS
Tester 3 Accuracy CPS
Tester 3 Hallucination CPS
Tester 3 Ground Truth Summary
No. No, I didn’t. Maybe that’s what I should do. I didn’t even think about that. You know, I just never had no problems before.
problems with credit card
1
0
Caller states they’ve never had any credit‑card issues.
Okay. Oh, okay then. And thank you so very much, Jay. I know this. Thank you. You have a great day.
thank jay for a great day
5
0
Caller thanks Jay and ends the call warmly.
Yes, with whom I speaking? Jason, good morning. My name is birthday. Uh‑huh. Yes, I tried my son, some funds and
jason’s son, funds and funds
0
0
Caller asks for agent Jason and mentions attempting a funds transfer.
Remember the exact error message. 900. No. But as you know, I haven’t said anything in a long time. So No.
error message
2
0
Caller requests recall of the system error code “900.”
Oh. Yep, because it says something about later, you know. Okay. Okay. I wonder if you change thanks or something.
change thanks or something
1
0
Caller inquires about modifying a “thanks” message or setting.
Then if it’s going to take that long, I needed expedited because she told me I will be able to get it to.
expedite the delivery of a house
1
1
Caller requests expedited service due to delay.
Okay, okay, man, what’s that? … you said it’s being mailed out. expedited she said I would get it the next day. … can you tell me if it’s still possible before it to be expedited. …
expedite the delivery of a package
5
0
Caller asks to expedite the mailed item for next‑day delivery.
Okay. Bye. Yeah, yeah. Awesome. … So it’s coming UPS. … Thank you so much.
ups is coming
4
0
Caller confirms the package is arriving via UPS.
Arlithia Watson. Okay, I’m sorry. Can you repeat that? … I’m calling because I just left the branch … they froze my car and they were sending me out another
leave branch because it froze
1
1
Caller reports their card was frozen at the branch and they left.
Arlithia Watson. … they froze my car … sending me out another as yesterday … supposed to be able to get it expedited … 506 Truman … lithium waiting at Yahoo.
expedite a car being sent out
1
1
Caller asks to expedite sending a replacement card.
Okay, otherwise it’ll put a stop on it … But just for today, I couldn’t put a stop one just for today … because that 205 was done a lot of damage on my account today.
put a stop one on a car for today
0
1
Caller requests a stop payment on a $205 charge today.
John Davis. Thank you. Chairman, a stop payment on something that was taken out from my account. Is it too late?
stop payment on something taken out of account
5
0
Caller asks if they can still stop a payment already debited.







cd /app/cortex/dev1/shob
du -h --max-depth=1 . 2>/dev/null | sort -hr



du -h /app/cortex/dev1/shob 2>/dev/null \
  | sort -hr \
  | head -n 10
du -sh /app/cortex/dev1/shob/.local/Trash

du -h --max-depth=1 /app/cortex/dev1/shob/.local/Trash


du -h --max-depth=2 /app/cortex/dev1/shob/.local/Trash/files | sort -h


# 1. (Optional) Double‑check what you're about to delete:
ls -lah /app/cortex/dev1/shob/.local/Trash/files
ls -lah /app/cortex/dev1/shob/.local/Trash/info

# 2. Delete all trashed files and metadata:
rm -rf /app/cortex/dev1/shob/.local/Trash/files/* 
rm -rf /app/cortex/dev1/shob/.local/Trash/info/*

# 3. (Optional) Verify it’s empty:
du -sh /app/cortex/dev1/shob/.local/Trash



find "$HOME" -type d \( -iname "*trash*" -o -iname ".Trash*" \)

Baseline (no warmup, no smoothing)
it drops “are we” because the first big updates scramble small function words.




2) Add warmup_steps=500
	•	In plain terms:
You don’t shove the bike forward at full speed—you steady the learner with small pushes first.
	•	Effect:
The model’s first 500 steps use a very low LR, so it learns those tiny helper words before speeding up.

now “are we” is back, though the main verb still isn’t quite right.

⸻

3) Also add label_smoothing_factor=0.1
	•	In plain terms:
Instead of saying “I want exactly this word,” you say “I want mostly this word, but maybe a tiny bit of other words too.”
	•	Effect:
The model stops over‑committing to the exact training phrasing, so it chooses the correct verb form and natural word order.
	•	Final output:


perfect match, with both the auxiliary (“are we”) and the gerund (“requesting”) correctly placed.

⸻

Bottom line:
	•	Warmup brings back tiny but crucial words (“are we”).
	•	Smoothing polishes the verb form and phrasing (“requesting”).

Together they turn

“Why request this information?”
into
“Why are we requesting this information?”



With our new warmup_steps=500 it’ll ramp the learning‑rate slowly, so early on it won’t overshoot and drop words.
	•	And label_smoothing_factor=0.1 keeps it from becoming over‑confident on any single token.


 args = Seq2SeqTrainingArguments(
     output_dir="…",
     do_train=True,
     eval_steps=100,
     learning_rate=1.2453e-6,
     per_device_train_batch_size=batch_size,
     per_device_eval_batch_size=batch_size,
     weight_decay=0.00227,
     save_total_limit=1,
     num_train_epochs=14,
     predict_with_generate=True,
+    warmup_steps=500,              # ← slowly ramp up the LR over 500 steps
+    label_smoothing_factor=0.1,    # ← smooth the target distribution by 0.1
 )

import os
import warnings
import pandas as pd
import numpy as np
import torch
from datasets import Dataset
from nltk.translate.bleu_score import sentence_bleu
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback,
    logging,
)
from accelerate import Accelerator, DataLoaderConfiguration
from peft import LoraConfig, get_peft_model

# ─── Setup ──────────────────────────────────────────────────────────────────────
warnings.filterwarnings("ignore")
logging.set_verbosity_error()
torch.manual_seed(0)
os.environ["CUDA_VISIBLE_DEVICES"] = "1,2"
print("GPUs available:", torch.cuda.device_count())

# ─── Hyperparameters ────────────────────────────────────────────────────────────
checkpoint                   = "/appdata/cortex/dev1/aptaiModels/madlad400-3b-mt"
output_dir                   = "/appdata/cortex/dev1/shob/Refined_datasets"
max_length                   = 128
batch_size                   = 8
gradient_accumulation_steps  = 4
learning_rate                = 1.245e-06
weight_decay                 = 0.00227
num_train_epochs             = 14
eval_steps                   = 100
warmup_ratio                 = 0.1
label_smoothing_factor       = 0.1
lr_scheduler_type            = "cosine"
bleu_workers                 = 25

# ─── Load Model & Tokenizer ─────────────────────────────────────────────────────
tokenizer  = T5Tokenizer.from_pretrained(checkpoint)
base_model = T5ForConditionalGeneration.from_pretrained(
    checkpoint,
    low_cpu_mem_usage=True,
    offload_state_dict=True
)

# ─── Data Collator ───────────────────────────────────────────────────────────────
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=base_model,
    padding=True,
    pad_to_multiple_of=8
)

# ─── Load & Prepare Datasets ────────────────────────────────────────────────────
def load_lower_df(path):
    df = pd.read_excel(path)
    return df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

train_df = load_lower_df(f"{output_dir}/refined_train_df.xlsx")
val_df   = load_lower_df(f"{output_dir}/validation_df.xlsx")

train_ds = Dataset.from_pandas(train_df)
eval_ds  = Dataset.from_pandas(val_df)

def preprocess_function(examples):
    inputs  = examples["es"]
    targets = examples["en"]
    model_inputs = tokenizer(
        inputs,
        text_target=targets,
        max_length=max_length,
        truncation=True
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=max_length,
            truncation=True
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_ds.map(
    preprocess_function,
    batched=True,
    remove_columns=["es","en"]
)
tokenized_eval = eval_ds.map(
    preprocess_function,
    batched=True,
    remove_columns=["es","en"]
)

# ─── Attach LoRA Adapters ───────────────────────────────────────────────────────
lora_config = LoraConfig(
    r               = 32,
    lora_alpha      = 64,
    target_modules  = ["q_proj","v_proj","k_proj"],
    lora_dropout    = 0.2,
    bias            = "all",        # fine‑tune all bias terms
    fan_in_fan_out  = True          # match model’s internal weight layout
)
model = get_peft_model(base_model, lora_config)
model.gradient_checkpointing_enable()

# ─── BLEU Metric Functions ─────────────────────────────────────────────────────
def postprocess_text(preds, labels):
    preds  = [p.strip() for p in preds]
    labels = [[l.strip()] for l in labels]
    return preds, labels

def get_transMetrics_single(t, r):
    bleu = sentence_bleu([r.split()], t.split(), weights=(0.75,0.25,0,0))
    return {"BLEU": round(bleu,2)}

def get_transMetrics_batch(translation, reference, workers=bleu_workers):
    flat_ref = (
        [item for sublist in reference for item in sublist]
        if isinstance(reference[0], list) else reference
    )
    with ThreadPoolExecutor(max_workers=workers) as exe:
        futures = [exe.submit(get_transMetrics_single, t, r)
                   for t, r in zip(translation, flat_ref)]
        results = [f.result() for f in as_completed(futures)]
    return pd.DataFrame(results).mean().round(2).to_dict()

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds  = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
    return get_transMetrics_batch(decoded_preds, decoded_labels)

# ─── Accelerator & Dataloader Config ──────────────────────────────────────────
dataloader_config = DataLoaderConfiguration(dispatch_batches=False, split_batches=True)
accelerator = Accelerator(
    device_placement            = True,
    gradient_accumulation_steps = gradient_accumulation_steps,
    mixed_precision             = "fp16",
    dataloader_config           = dataloader_config
)

# ─── TrainingArguments & Trainer ───────────────────────────────────────────────
training_args = Seq2SeqTrainingArguments(
    output_dir                  = output_dir,
    do_train                    = True,
    evaluation_strategy         = "steps",
    eval_steps                  = eval_steps,
    save_strategy               = "steps",
    save_steps                  = eval_steps,
    load_best_model_at_end      = True,
    metric_for_best_model       = "BLEU",
    greater_is_better           = True,
    learning_rate               = learning_rate,
    per_device_train_batch_size = batch_size,
    per_device_eval_batch_size  = batch_size,
    weight_decay                = weight_decay,
    warmup_ratio                = warmup_ratio,
    lr_scheduler_type           = lr_scheduler_type,
    label_smoothing_factor      = label_smoothing_factor,
    num_train_epochs            = num_train_epochs,
    predict_with_generate       = True,
    logging_strategy            = "steps",
    logging_steps               = 50,
    save_total_limit            = 2,
)

trainer = Seq2SeqTrainer(
    model           = model,
    args            = training_args,
    train_dataset   = tokenized_train,
    eval_dataset    = tokenized_eval,
    data_collator   = data_collator,
    tokenizer       = tokenizer,
    compute_metrics = compute_metrics,
    callbacks       = [EarlyStoppingCallback(early_stopping_patience=3)],
)

# ─── Prepare & Start Training ──────────────────────────────────────────────────
trainer = accelerator.prepare(trainer)
result  = trainer.train()
print(result)