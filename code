Hi Team,

I wanted to let you know that I’ve updated our BERTScore evaluation pipeline to use HuggingFace’s built‑in baseline rescaling instead of manually hard‑coding a baseline value. Below is a quick overview of what’s changed, why it’s better, and two small examples—one for our English “above” evaluation and one for the Spanish model report.

⸻

1. What changed and why
	•	Old approach: We subtracted a fixed baseline (e.g., 0.7) and linearly scaled scores to [0,1]. This required maintaining an external constant and risked mis‑scaling if the true “random” baseline shifted by model or language.
	•	New approach: We simply pass rescale_with_baseline=True (plus the appropriate lang code) to scorer.compute(). HuggingFace automatically estimates the reference‑vs‑reference baseline for that language and model, then normalizes all Precision, Recall, and F1 scores into [0,1]. This ensures consistency across models and avoids any manual constants.

⸻

2. How rescale_with_baseline works
	1.	For a given language/model, HF computes the average BERTScore between identical reference sentences (“reference → reference baseline”).
	2.	It subtracts that baseline from each raw score and divides by (1 − baseline).
	3.	The result is guaranteed to lie between 0 (no better than random) and 1 (perfect match).

Here, instead of (raw_score – 0.7)/(1–0.7), HF’s baseline might be ~0.38, so (raw_score – 0.38)/0.62 is applied automatically.

By using lang="es", the baseline is estimated on Spanish references, giving us true 0–1 scores that reflect model quality, not arbitrary thresholds.

⸻

Please let me know if you have any questions or need additional examples. Thanks!

Best regards,
Shobharani Polasa
