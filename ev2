Now explain me clearly batch size learning rate warmup and label smoothing factor with one input example please and expected output


batch_size = 16

✅ What it means:

You process 16 examples at once before updating the model weights.

⸻

🧠 Why 16?
	•	It’s big enough to get a stable average of gradients across different examples.
	•	It’s small enough to fit on most modern GPUs (especially with LoRA).

⸻

✅ Expected Behavior:
	•	For every 16 sentences (like the example), the model:
	1.	Tokenizes each
	2.	Predicts output tokens for each
	3.	Calculates loss for each token in each sentence
	4.	Averages the losses across all 16 samples
	5.	Updates model weights once

⸻

📈 Benefits:
	•	Faster training (parallelism)
	•	More stable gradient updates than batch size = 1

⚠️ Downsides:
	•	Too large? GPU might run out of memory
	•	Too small? Updates become noisy

⸻

🔷 2. learning_rate = 2e-4 (0.0002)

✅ What it means:

This is how big a step the model takes when updating weights after each batch.

⸻

🧠 Why 2e-4?
	•	It’s high enough to make noticeable progress (fine-tuning)
	•	But not too high, so it won’t destabilize T5’s pretrained weights
