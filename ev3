import time
import pandas as pd
from sacrebleu import sentence_bleu, corpus_bleu, corpus_chrf, sentence_chrf
from typing import Union, List, Optional, Dict
import warnings
from pathlib import Path
from sacrebleu.metrics import CHRF
import os

class TranslationEvaluator:
    """
    Load data
    BLEU, chrf, Chrf++, COMET metrics
    summary
    excel report generation
    """
    def __init__(
        self,
        output_file: Union[str, Path] = "output_data.xlsx"
    ):
        os.environ["Transformers cache"] = r'C:/Users/ZKC7H0U/Documents/4. BERT score/roberta-large'
        warnings.simplefilter('ignore')

        self._data = None
        self._detailed_results = None
        self._model_metrics = None
        self._output_file = Path(output_file)

        # chrF++ object (char+up-to-bigram word n-grams)
        self._chrff = CHRF(word_order=2)

    def load_data(self, file_path: Union[str, Path]) -> None:
        """Load file (CSV or Excel) into self._data"""
        file_path = Path(file_path)
        if file_path.suffix == '.csv':
            self._data = pd.read_csv(file_path)
        elif file_path.suffix in ('.xls', '.xlsx'):
            self._data = pd.read_excel(file_path)
        else:
            raise ValueError("unsupported file type")

    def evaluate(
        self,
        prediction_cols: List[str],
        reference_col: str = "en",
        metrics: List[str] = ["BLEU", "ChrF", "ChrF++", "COMET"],
        keep_cols: Optional[List[str]] = None
    ) -> None:
        """
        Run all requested metrics.
        - prediction_cols: list of your model-output columns
        - reference_col: the ground-truth column
        - metrics: subset of ["BLEU","ChrF","ChrF++","COMET"]
        - keep_cols: any other columns (e.g. IDs, source) to carry through
        """
        if self._data is None:
            raise ValueError("No data loaded")

        keep_cols = keep_cols or []
        if reference_col not in keep_cols:
            keep_cols.insert(0, reference_col)

        self._validate_columns(reference_col, prediction_cols, keep_cols)
        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep_cols)
        self._compute_model_metrics(reference_col, prediction_cols, metrics)

    def _validate_columns(
        self,
        reference_col: str,
        prediction_cols: List[str],
        keep_cols: List[str]
    ) -> None:
        """Ensure all specified columns exist in the DataFrame"""
        all_columns = [reference_col] + prediction_cols + keep_cols
        missing = [col for col in all_columns if col not in self._data.columns]
        if missing:
            raise ValueError(f"columns not found in data: {missing}")

    def _compute_detailed_metrics(
        self,
        reference_col: str,
        prediction_cols: List[str],
        metrics: List[str],
        keep_cols: List[str]
    ) -> None:
        """Compute sentence-level metrics and store in self._detailed_results"""
        cols_to_keep = keep_cols.copy()
        if reference_col not in cols_to_keep:
            cols_to_keep.insert(0, reference_col)

        df = self._data.copy()
        for pred_col in prediction_cols:
            if "BLEU" in metrics:
                df[f"{pred_col} BLEU"] = df.apply(
                    lambda row: sentence_bleu(row[pred_col], [row[reference_col]]).score,
                    axis=1
                )
            if "ChrF" in metrics:
                df[f"{pred_col} ChrF"] = df.apply(
                    lambda row: sentence_chrf(row[pred_col], [row[reference_col]]).score,
                    axis=1
                )
            if "ChrF++" in metrics:
                df[f"{pred_col} ChrF++"] = df.apply(
                    lambda row: self._chrff.sentence_score(row[pred_col], [row[reference_col]]).score,
                    axis=1
                )

        # Reorder columns: keep_cols | prediction | metrics...
        new_cols = cols_to_keep.copy()
        for pred_col in prediction_cols:
            new_cols.append(pred_col)
            if "BLEU" in metrics:
                new_cols.append(f"{pred_col} BLEU")
            if "ChrF" in metrics:
                new_cols.append(f"{pred_col} ChrF")
            if "ChrF++" in metrics:
                new_cols.append(f"{pred_col} ChrF++")

        self._detailed_results = df[new_cols]

    def _compute_model_metrics(
        self,
        reference_col: str,
        prediction_cols: List[str],
        metrics: List[str]
    ) -> None:
        """
        Compute corpus-level and macro-average scores for each prediction column,
        storing results in self._model_metrics.
        """
        results = []
        # must have detailed_results already computed
        for pred_col in prediction_cols:
            refs = self._data[reference_col].tolist()
            hyps = self._data[pred_col].tolist()
            row = {"Model": pred_col}

            if "BLEU" in metrics:
                corpus_score = corpus_bleu(hyps, [refs]).score
                row["BLEU"] = corpus_score
                # macro-average of sentence-level BLEU
                row["BLEU_macro"] = self._detailed_results[f"{pred_col} BLEU"].mean()

            if "ChrF" in metrics:
                corpus_score = corpus_chrf(hyps, [refs]).score
                row["ChrF"] = corpus_score
                # macro-average of sentence-level ChrF
                row["ChrF_macro"] = self._detailed_results[f"{pred_col} ChrF"].mean()

            if "ChrF++" in metrics:
                corpus_score = self._chrff.corpus_score(hyps, [refs]).score
                row["ChrF++"] = corpus_score
                # macro-average of sentence-level ChrF++
                row["ChrF++_macro"] = self._detailed_results[f"{pred_col} ChrF++"].mean()

            results.append(row)

        self._model_metrics = pd.DataFrame(results)

    def _generate_report(self) -> None:
        """Write both detailed and model metrics to an Excel file"""
        out = self._output_file
        out.parent.mkdir(parents=True, exist_ok=True)
        with pd.ExcelWriter(out, engine='xlsxwriter') as writer:
            # detailed metrics sheet
            self._detailed_results.to_excel(
                writer, sheet_name="Detailed metrics", index=False
            )
            # model metrics sheet (with macro averages)
            self._model_metrics.to_excel(
                writer, sheet_name="Model metrics", index=False
            )

            wb = writer.book
            ws = writer.sheets["Model metrics"]
            max_row, max_col = self._model_metrics.shape
            cols = [{'header': c} for c in self._model_metrics.columns]
            ws.add_table(0, 0, max_row, max_col - 1, {
                'column': cols,
                'style': 'Table Style Medium 9',
                'name':  'ModelMetrics'
            })
            for i, col in enumerate(self._model_metrics.columns):
                width = max(
                    self._model_metrics[col].astype(str).map(len).max(),
                    len(col)
                ) + 2
                ws.set_column(i, i, width)

        print(f"Report saved to {out.resolve()}")

    def get_detailed_results(self) -> pd.DataFrame:
        """Retrieve the per-sentence metrics DataFrame"""
        return self._detailed_results.copy()

    def get_model_metrics(self) -> pd.DataFrame:
        """Retrieve the aggregate model metrics DataFrame"""
        return self._model_metrics.copy()