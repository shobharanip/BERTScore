import pandas as pd
from pandas.api import types as ptypes
from sacrebleu import sentence_bleu, sentence_chrf
from sacrebleu.metrics import CHRF
from pathlib import Path
from typing import Union, List, Optional
import warnings, os

class TranslationEvaluator:
    """
    Load data
    Compute sentence-level BLEU, ChrF, ChrF++
    Compute macro-average model metrics
    Write Detailed + Model metrics to Excel
    """
    def __init__(
        self,
        output_file: Union[str, Path] = "output_data.xlsx"
    ):
        os.environ["TRANSFORMERS_CACHE"] = r"C:/Users/You/.cache/huggingface"
        warnings.simplefilter("ignore")

        self._data             = None
        self._detailed_results = None
        self._model_metrics    = None
        self._output_file      = Path(output_file)
        self._chrff            = CHRF(word_order=2)

    def load_data(self, file_path: Union[str, Path]) -> None:
        """Load a CSV or Excel file into self._data."""
        fp = Path(file_path)
        if fp.suffix == ".csv":
            self._data = pd.read_csv(fp)
        elif fp.suffix in (".xls", ".xlsx"):
            self._data = pd.read_excel(fp)
        else:
            raise ValueError(f"Unsupported file type: {fp.suffix}")

    def evaluate(
        self,
        prediction_cols: Optional[List[str]] = None,
        reference_col: str = "en",
        metrics: List[str] = ["BLEU","ChrF","ChrF++"],
        keep_cols: Optional[List[str]] = None
    ) -> None:
        """
        Compute sentence-level metrics and macro-average summary.

        - prediction_cols: list of your model-output columns;
          if None, auto-select all string columns except 'es' and reference_col.
        - reference_col: ground-truth column (default 'en')
        - metrics: subset of ["BLEU","ChrF","ChrF++"]
        - keep_cols: extra columns to carry into the detailed sheet
        """
        if self._data is None:
            raise ValueError("No data loaded. Call load_data() first.")

        # -- auto-detect predictions if none provided --
        if prediction_cols is None:
            exclude = {reference_col, "es"}
            # only object (string) dtypes
            prediction_cols = [
                col for col in self._data.columns
                if ptypes.is_object_dtype(self._data[col]) and col not in exclude
            ]
            if not prediction_cols:
                raise ValueError(
                    f"No string columns available for predictions "
                    f"(excluding {exclude})."
                )

        # cast predictions + reference to str to avoid type errors
        for c in prediction_cols + [reference_col]:
            if c not in self._data.columns:
                raise ValueError(f"Column '{c}' not found in data.")
            self._data[c] = self._data[c].astype(str)

        # prepare keep_cols and ensure reference appears first
        keep = keep_cols.copy() if keep_cols else []
        if reference_col not in keep:
            keep.insert(0, reference_col)

        self._validate_columns(reference_col, prediction_cols, keep)
        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep)
        self._compute_model_metrics(metrics, prediction_cols)

    def _validate_columns(
        self,
        reference_col: str,
        prediction_cols: List[str],
        keep_cols: List[str]
    ) -> None:
        missing = [
            c for c in [reference_col] + prediction_cols + keep_cols
            if c not in self._data.columns
        ]
        if missing:
            raise ValueError(f"Columns not found in data: {missing}")

    def _compute_detailed_metrics(
        self,
        reference_col: str,
        prediction_cols: List[str],
        metrics: List[str],
        keep_cols: List[str]
    ) -> None:
        """Attach per-sentence BLEU, ChrF, ChrF++ into self._detailed_results."""
        df = self._data.copy()
        for p in prediction_cols:
            if "BLEU" in metrics:
                df[f"{p} BLEU"] = df.apply(
                    lambda r: sentence_bleu(r[p], [r[reference_col]]).score,
                    axis=1
                )
            if "ChrF" in metrics:
                df[f"{p} ChrF"] = df.apply(
                    lambda r: sentence_chrf(r[p], [r[reference_col]]).score,
                    axis=1
                )
            if "ChrF++" in metrics:
                df[f"{p} ChrF++"] = df.apply(
                    lambda r: self._chrff.sentence_score(r[p], [r[reference_col]]).score,
                    axis=1
                )

        # reorder: keep_cols | each pred + its metrics
        cols = keep_cols.copy()
        for p in prediction_cols:
            cols.append(p)
            for m in ["BLEU","ChrF","ChrF++"]:
                if m in metrics:
                    cols.append(f"{p} {m}")

        self._detailed_results = df[cols]

    def _compute_model_metrics(
        self,
        metrics: List[str],
        prediction_cols: List[str]
    ) -> None:
        """Compute macro-average of each sentence-level metric."""
        rows = []
        for p in prediction_cols:
            row = {"Model": p}
            for m in ["BLEU","ChrF","ChrF++"]:
                if m in metrics:
                    col_name = f"{p} {m}"
                    row[m] = self._detailed_results[col_name].mean()
            rows.append(row)
        self._model_metrics = pd.DataFrame(rows)

    def generate_report(self) -> None:
        """
        Write two sheets to Excel:
         - Detailed metrics
         - Model metrics (macro-average only)
        Prints the exact path so you know where it saved.
        """
        self._output_file.parent.mkdir(parents=True, exist_ok=True)
        with pd.ExcelWriter(self._output_file, engine="xlsxwriter") as writer:
            self._detailed_results.to_excel(
                writer, sheet_name="Detailed metrics", index=False
            )
            self._model_metrics.to_excel(
                writer, sheet_name="Model metrics", index=False
            )

            # format Model metrics sheet
            wb = writer.book
            ws = writer.sheets["Model metrics"]
            max_row, max_col = self._model_metrics.shape
            headers = [{"header": c} for c in self._model_metrics.columns]
            ws.add_table(0, 0, max_row, max_col - 1, {
                "column": headers,
                "style": "Table Style Medium 9",
                "name": "ModelMetrics"
            })
            for idx, col in enumerate(self._model_metrics.columns):
                width = max(
                    self._model_metrics[col].astype(str).map(len).max(),
                    len(col)
                ) + 2
                ws.set_column(idx, idx, width)

        print(f"Report saved to: {self._output_file.resolve()}")

    def get_detailed_results(self) -> pd.DataFrame:
        return self._detailed_results.copy()

    def get_model_metrics(self) -> pd.DataFrame:
        return self._model_metrics.copy()