# translation_app.py

import os
import torch
import traceback
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import T5ForConditionalGeneration, T5Tokenizer
from peft import PeftModel

# ------------------------------------------------------------------------------
# 1. DEVICE and MODEL PATHS
# ------------------------------------------------------------------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Base T5 model (Madlad400)
BASE_MODEL_PATH = os.getenv(
    "BASE_MODEL",
    "/appdata/cortex/dev1/aptaiModels/madlad400-3b-mt"
)

# LoRA adapter directory
LORA_ADAPTER_PATH = os.getenv(
    "LORA_PATH",
    "/appdata/cortex/dev1/shob/refined_datasets/madlad400_finetuned_LORA_rank64alpha64"
)

# ------------------------------------------------------------------------------
# 2. LOAD TOKENIZER & BASE MODEL
# ------------------------------------------------------------------------------
try:
    tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH)
    base_model = T5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)
except Exception as e:
    raise RuntimeError(
        f"Error loading base model/tokenizer from '{BASE_MODEL_PATH}': {e}"
    )

# ------------------------------------------------------------------------------
# 3. LOAD LoRA ADAPTER
# ------------------------------------------------------------------------------
try:
    model = PeftModel.from_pretrained(
        base_model,
        LORA_ADAPTER_PATH,
        local_files_only=True,
        repo_type="model"
    )
    model.to(DEVICE)
    model.eval()
except Exception as e:
    raise RuntimeError(
        f"Error loading LoRA adapter from '{LORA_ADAPTER_PATH}': {e}"
    )

# ------------------------------------------------------------------------------
# 4. FASTAPI SETUP
# ------------------------------------------------------------------------------
app = FastAPI(
    title="Spanish→English Translation",
    description="Translate Spanish text into English using a LoRA-fine-tuned Madlad400 T5 model",
    version="1.0.0"
)

class TranslateRequest(BaseModel):
    spanish: str

class TranslateResponse(BaseModel):
    english: str

# ------------------------------------------------------------------------------
# 5. HEALTH‐CHECK ENDPOINT
# ------------------------------------------------------------------------------
@app.get("/")
async def health_check():
    return {
        "status": "healthy" if (tokenizer and model) else "failed",
        "device": DEVICE,
        "base_model": BASE_MODEL_PATH,
        "lora_adapter": LORA_ADAPTER_PATH
    }

# ------------------------------------------------------------------------------
# 6. TRANSLATION ENDPOINT
# ------------------------------------------------------------------------------
@app.post("/translate/", response_model=TranslateResponse)
async def translate(request: TranslateRequest):
    text = request.spanish.strip()
    if not text:
        raise HTTPException(400, "Input Spanish text must be non-empty")

    # T5 prefix must match what you used during fine-tuning
    prompt = "translate Spanish to English: " + text

    try:
        # Tokenize
        inputs = tokenizer.encode(
            prompt,
            return_tensors="pt",
            truncation=True
        ).to(DEVICE)

        # Generate (adjust max_length/num_beams as needed)
        outputs = model.generate(
            inputs,
            max_length=512,
            num_beams=4,
            early_stopping=True
        )

        # Decode
        english = tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )

        return TranslateResponse(english=english)

    except Exception as e:
        tb = traceback.format_exc()
        # (Optionally log tb somewhere)
        raise HTTPException(500, f"Translation failed: {e}")

# ------------------------------------------------------------------------------
# 7. RUN WITH UVICORN
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "translation_app:app",
        host="0.0.0.0",
        port=8001,
        reload=True
    )