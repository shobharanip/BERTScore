
pip install fastapi uvicorn transformers torch peft
# or (if you used Conda):
conda install -c conda-forge fastapi uvicorn
pip install transformers torch peft



{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_evaluator.py\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, sentence_chrf2\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    - load_data(): CSV or Excel\n",
    "    - evaluate(): computes per-model & per-metric columns (plus optional per-model timing)\n",
    "    - get_detailed_results(): DataFrame with those columns\n",
    "    - get_model_metrics(): dict of all metric means\n",
    "    - get_response_times(): DataFrame of run_id vs avg_response_time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data: pd.DataFrame | None = None\n",
    "        self._detailed_results: pd.DataFrame = pd.DataFrame()\n",
    "        self._model_metrics: dict[str, float] = {}\n",
    "        self._response_times: pd.DataFrame = pd.DataFrame(\n",
    "            [], columns=[\"run_id\", \"avg_response_time\"]\n",
    "        )\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        p = Path(file_path)\n",
    "        if p.suffix.lower() == \".csv\":\n",
    "            self._data = pd.read_csv(p)\n",
    "        elif p.suffix.lower() in (\".xls\", \".xlsx\"):\n",
    "            self._data = pd.read_excel(p)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format (must be .csv, .xls or .xlsx)\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col:    str,\n",
    "        metrics:          List[str],\n",
    "        keep_cols:        List[str],\n",
    "        run_id:           int | None = None,\n",
    "        measure_time:     bool      = False,\n",
    "    ) -> None:\n",
    "        if self._data is None:\n",
    "            raise RuntimeError(\"No data loaded; call load_data() first.\")\n",
    "\n",
    "        rows = []\n",
    "        time_records: list[float] = []\n",
    "\n",
    "        for _, r in self._data.iterrows():\n",
    "            rec = {c: r[c] for c in keep_cols}\n",
    "            ref = str(r[reference_col])\n",
    "\n",
    "            for col in prediction_cols:\n",
    "                pred = str(r[col])\n",
    "                if measure_time:\n",
    "                    start = time.time()\n",
    "\n",
    "                # compute each metric and store under \"{col} {METRIC}\"\n",
    "                for m in metrics:\n",
    "                    m_up = m.upper()\n",
    "                    if m_up == \"BLEU\":\n",
    "                        sc = sentence_bleu(pred, [ref]).score\n",
    "                    elif m_up == \"CHRF\":\n",
    "                        sc = sentence_chrf(pred, [ref]).score\n",
    "                    elif m_up in (\"CHRF++\", \"CHRFF++\", \"CHR F++\"):\n",
    "                        sc = sentence_chrf2(pred, [ref]).score\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported metric: {m!r}\")\n",
    "                    rec[f\"{col} {m_up}\"] = sc\n",
    "\n",
    "                # optional timing column per model\n",
    "                if measure_time and run_id is not None:\n",
    "                    elapsed = time.time() - start\n",
    "                    rec[f\"{col} Response Time Run {run_id}\"] = elapsed\n",
    "                    time_records.append(elapsed)\n",
    "\n",
    "            rows.append(rec)\n",
    "\n",
    "        # build detailed-results DataFrame\n",
    "        self._detailed_results = pd.DataFrame(rows)\n",
    "\n",
    "        # aggregate model metrics: mean of every \"{col} {METRIC}\"\n",
    "        metric_cols = [f\"{c} {m.upper()}\" for c in prediction_cols for m in metrics]\n",
    "        self._model_metrics = {\n",
    "            col: float(self._detailed_results[col].mean()) for col in metric_cols\n",
    "        }\n",
    "\n",
    "        # if timing was measured, record the run average\n",
    "        if measure_time and run_id is not None:\n",
    "            avg = float(np.mean(time_records))\n",
    "            new = pd.DataFrame([{\"run_id\": run_id, \"avg_response_time\": avg}])\n",
    "            self._response_times = pd.concat(\n",
    "                [self._response_times, new], ignore_index=True\n",
    "            )\n",
    "\n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        return self._detailed_results.copy()\n",
    "\n",
    "    def get_model_metrics(self) -> dict[str, float]:\n",
    "        return dict(self._model_metrics)\n",
    "\n",
    "    def get_response_times(self) -> pd.DataFrame:\n",
    "        return self._response_times.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation_evaluator import TranslationEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    evaluator = TranslationEvaluator()\n",
    "    evaluator.load_data(\"input_data.xlsx\")\n",
    "\n",
    "    models   = ['base_madlad400_translation', 'finetuned_madlad400_translation']\n",
    "    keep     = [\"en\", \"es\"]\n",
    "    ref_col  = \"en\"\n",
    "    metrics  = [\"BLEU\", \"ChrF\", \"ChrF++\"]\n",
    "\n",
    "    # Run #1, with timing\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=1,\n",
    "        measure_time=True\n",
    "    )\n",
    "\n",
    "    # Run #2, without timing\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=2,\n",
    "        measure_time=False\n",
    "    )\n",
    "\n",
    "    # View detailed results\n",
    "    df = evaluator.get_detailed_results()\n",
    "    print(\"Columns in detailed results:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Example: select exactly the columns you need:\n",
    "    want = [\n",
    "      'base_madlad400_translation BLEU',\n",
    "      'base_madlad400_translation CHRF',\n",
    "      'base_madlad400_translation Response Time Run 1',\n",
    "      # etc. for the rest...\n",
    "    ]\n",
    "    print(df[want].head(), \"\\n\")\n",
    "\n",
    "    # View aggregated metrics\n",
    "    print(\"Aggregated metrics:\")\n",
    "    print(pd.DataFrame([evaluator.get_model_metrics()]), \"\\n\")\n",
    "\n",
    "    # View timing table\n",
    "    print(\"Response times by run:\")\n",
    "    print(evaluator.get_response_times())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation_evaluator_Research.py\n",
    "\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, corpus_bleu, sentence_chrf, corpus_chrf\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, output_file: Union[str, Path] = \"output_data.xlsx\"):\n",
    "        \"\"\"\n",
    "        :param output_file: where to write the Excel report\n",
    "        \"\"\"\n",
    "        self._data: Optional[pd.DataFrame] = None\n",
    "        self._detailed_results: Optional[pd.DataFrame] = None\n",
    "        self._model_metrics: Optional[pd.DataFrame] = None\n",
    "        self._output_file = Path(output_file)\n",
    "\n",
    "        # suppress any tokenizer warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load your input_data.xlsx / .csv into a DataFrame.\n",
    "        Expects at least one column for references (e.g. \"en\") and\n",
    "        one or more columns of model outputs.\n",
    "        \"\"\"\n",
    "        p = Path(file_path)\n",
    "        if p.suffix.lower() in {\".xls\", \".xlsx\"}:\n",
    "            df = pd.read_excel(p)\n",
    "        elif p.suffix.lower() == \".csv\":\n",
    "            df = pd.read_csv(p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported extension: {p.suffix}\")\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Loaded DataFrame is empty.\")\n",
    "        self._data = df\n",
    "        return df\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col: str = \"en\",\n",
    "        metrics: List[str] = [\"BLEU\", \"CHRF\", \"ChrF++\"],\n",
    "        keep_cols: Optional[List[str]] = None,\n",
    "        measure_time: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Compute per-sentence and corpus-level BLEU, CHRF and ChrF++.\n",
    "        :param prediction_cols: list of your model-output column names\n",
    "        :param reference_col: the column with your human/reference translation\n",
    "        :param metrics: choose any of \"BLEU\", \"CHRF\", \"ChrF++\"\n",
    "        :param keep_cols: any additional columns (e.g. \"es\") you want carried into the Detailed sheet\n",
    "        :param measure_time: if True, also record per-sentence latency\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            raise ValueError(\"Please call load_data() first.\")\n",
    "\n",
    "        df = self._data.copy()\n",
    "        keep_cols = keep_cols or []\n",
    "\n",
    "        # sanity‐check that all required columns exist\n",
    "        for col in [reference_col] + prediction_cols + keep_cols:\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"Column not found: {col}\")\n",
    "\n",
    "        detailed_records = []\n",
    "        # 1) per-sentence scores (+ optional timing)\n",
    "        for model in prediction_cols:\n",
    "            for _, row in df.iterrows():\n",
    "                hyp = str(row[model])\n",
    "                ref = str(row[reference_col])\n",
    "                rec = {\"model\": model}\n",
    "                if measure_time:\n",
    "                    t0 = time.time()\n",
    "\n",
    "                if \"BLEU\" in metrics:\n",
    "                    rec[\"BLEU\"] = sentence_bleu(hyp, [ref]).score\n",
    "                if \"CHRF\" in metrics:\n",
    "                    rec[\"CHRF\"] = sentence_chrf(hyp, [ref]).score\n",
    "                if \"ChrF++\" in metrics:\n",
    "                    # ChrF++ = word_order=2\n",
    "                    rec[\"ChrF++\"] = sentence_chrf(hyp, [ref], word_order=2).score\n",
    "\n",
    "                if measure_time:\n",
    "                    rec[\"response_time\"] = time.time() - t0\n",
    "\n",
    "                # carry over any additional columns (e.g. \"es\")\n",
    "                for col in keep_cols:\n",
    "                    rec[col] = row[col]\n",
    "\n",
    "                # also include the reference text if you like\n",
    "                rec[reference_col] = ref\n",
    "                detailed_records.append(rec)\n",
    "\n",
    "        self._detailed_results = pd.DataFrame(detailed_records)\n",
    "\n",
    "        # 2) corpus-level (aggregate) metrics\n",
    "        summary = []\n",
    "        for model in prediction_cols:\n",
    "            hyps = df[model].astype(str).tolist()\n",
    "            refs = df[reference_col].astype(str).tolist()\n",
    "            rec = {\"model\": model}\n",
    "            if \"BLEU\" in metrics:\n",
    "                rec[\"BLEU\"] = corpus_bleu(hyps, [refs]).score\n",
    "            if \"CHRF\" in metrics:\n",
    "                rec[\"CHRF\"] = corpus_chrf(hyps, [refs]).score\n",
    "            if \"ChrF++\" in metrics:\n",
    "                rec[\"ChrF++\"] = corpus_chrf(hyps, [refs], word_order=2).score\n",
    "            if measure_time:\n",
    "                times = self._detailed_results.query(\"model == @model\")[\"response_time\"]\n",
    "                rec[\"avg_response_time\"] = times.mean()\n",
    "            summary.append(rec)\n",
    "\n",
    "        self._model_metrics = pd.DataFrame(summary)\n",
    "\n",
    "        # 3) write them both to an Excel file\n",
    "        with pd.ExcelWriter(self._output_file) as writer:\n",
    "            self._detailed_results.to_excel(\n",
    "                writer, sheet_name=\"Detailed\", index=False\n",
    "            )\n",
    "            self._model_metrics.to_excel(\n",
    "                writer, sheet_name=\"Summary\", index=False\n",
    "            )\n",
    "\n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        if self._detailed_results is None:\n",
    "            raise ValueError(\"No detailed results – did you call evaluate()?\")\n",
    "        return self._detailed_results\n",
    "\n",
    "    def get_model_metrics(self) -> pd.DataFrame:\n",
    "        if self._model_metrics is None:\n",
    "            raise ValueError(\"No model metrics – did you call evaluate()?\")\n",
    "        return self._model_metrics\n",
    "\n",
    "    def get_response_times(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Only valid if you passed measure_time=True to evaluate().\n",
    "        \"\"\"\n",
    "        if self._detailed_results is None or \"response_time\" not in self._detailed_results:\n",
    "            raise ValueError(\"No timing data – did you evaluate(measure_time=True)?\")\n",
    "        return self._detailed_results[[\"model\", \"response_time\"]]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- example usage ---\n",
    "    evaluator = TranslationEvaluator(output_file=\"bert_translations_report.xlsx\")\n",
    "    evaluator.load_data(\"input_data.xlsx\")\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=[\n",
    "            \"base_madlad400_translation\",\n",
    "            \"finetuned_madlad400_translation\",\n",
    "            \"finetuned_helsinki_translation\",\n",
    "            \"base_helsinki_translation\",\n",
    "        ],\n",
    "        reference_col=\"en\",\n",
    "        metrics=[\"BLEU\", \"CHRF\", \"ChrF++\"],\n",
    "        keep_cols=[\"es\"],\n",
    "        measure_time=True,      # set False if you don't need per-sentence timing\n",
    "    )\n",
    "\n",
    "    print(\"=== Detailed results (first 5 rows) ===\")\n",
    "    print(evaluator.get_detailed_results().head(), \"\\n\")\n",
    "\n",
    "    print(\"=== Summary (corpus-level) ===\")\n",
    "    print(evaluator.get_model_metrics(), \"\\n\")\n",
    "\n",
    "    print(\"=== Response times ===\")\n",
    "    print(evaluator.get_response_times().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# translation_evaluator.py\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sacrebleu.metrics import CHRF\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, sentence_chrf2, corpus_bleu, corpus_chrf\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    Load bilingual data, compute BLEU/ChrF/ChrF++ and latency for two runs,\n",
    "    and write Summary / Detailed / Timings sheets to Excel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reference_col: str = \"en\"):\n",
    "        self.reference_col = reference_col\n",
    "        self.data: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "        # Will be filled during evaluate():\n",
    "        self._detailed_results: pd.DataFrame = pd.DataFrame()\n",
    "        self._all_response_times: pd.DataFrame = pd.DataFrame()\n",
    "        self._summary_df: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        p = Path(file_path)\n",
    "        if p.suffix.lower() in (\".csv\",):\n",
    "            self.data = pd.read_csv(p)\n",
    "        elif p.suffix.lower() in (\".xls\", \".xlsx\"):\n",
    "            self.data = pd.read_excel(p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {p.suffix}\")\n",
    "\n",
    "        if self.reference_col not in self.data.columns:\n",
    "            raise ValueError(f\"Reference column '{self.reference_col}' not found\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        metrics: List[str] = [\"BLEU\", \"ChrF\", \"ChrF++\"],\n",
    "        runs: List[int] = [1, 2],\n",
    "    ) -> None:\n",
    "        \"\"\"Compute sentence‑level metrics and timing for each model & run.\"\"\"\n",
    "        if self.data.empty:\n",
    "            raise ValueError(\"No data loaded – call load_data() first\")\n",
    "\n",
    "        detailed_rows = []\n",
    "        timing_rows = []\n",
    "\n",
    "        for run_id in runs:\n",
    "            for model in prediction_cols:\n",
    "                if model not in self.data.columns:\n",
    "                    raise ValueError(f\"Prediction column '{model}' missing\")\n",
    "\n",
    "                ref_sents = self.data[self.reference_col].astype(str).tolist()\n",
    "                sys_sents = self.data[model].astype(str).tolist()\n",
    "\n",
    "                # time the sentence‑by‑sentence loop\n",
    "                t0 = time.perf_counter()\n",
    "                for src_idx, (r, h) in enumerate(zip(ref_sents, sys_sents), start=1):\n",
    "                    row = {\n",
    "                        \"src\": self.data.index[src_idx - 1],\n",
    "                        \"ref\": r,\n",
    "                        \"Model\": model,\n",
    "                        \"Run\": run_id,\n",
    "                    }\n",
    "                    if \"BLEU\" in metrics:\n",
    "                        row[\"BLEU\"] = sentence_bleu(h, [r]).score\n",
    "                    if \"ChrF\" in metrics:\n",
    "                        row[\"ChrF\"] = sentence_chrf(h, [r]).score\n",
    "                    if \"ChrF++\" in metrics:\n",
    "                        row[\"ChrF++\"] = sentence_chrf2(h, [r]).score\n",
    "\n",
    "                    detailed_rows.append(row)\n",
    "                t1 = time.perf_counter()\n",
    "\n",
    "                avg_time = (t1 - t0) / len(ref_sents)\n",
    "                timing_rows.append({\n",
    "                    \"Model\": model,\n",
    "                    \"Run\": run_id,\n",
    "                    \"Average response time (s)\": avg_time\n",
    "                })\n",
    "\n",
    "        # Build DataFrames\n",
    "        self._detailed_results = pd.DataFrame(detailed_rows)\n",
    "        self._all_response_times = pd.DataFrame(timing_rows)\n",
    "\n",
    "        # Build Summary: run‑level corpus metrics + avg latency\n",
    "        summary_rows = []\n",
    "        for run_id in runs:\n",
    "            sub = self._detailed_results.query(f\"Run=={run_id}\")\n",
    "            for model in prediction_cols:\n",
    "                subm = sub[sub[\"Model\"] == model]\n",
    "                row = {\"Model\": model, \"Run\": run_id}\n",
    "                if \"BLEU\" in metrics:\n",
    "                    row[\"BLEU\"] = subm[\"BLEU\"].mean()\n",
    "                if \"ChrF\" in metrics:\n",
    "                    row[\"ChrF\"] = subm[\"ChrF\"].mean()\n",
    "                if \"ChrF++\" in metrics:\n",
    "                    row[\"ChrF++\"] = subm[\"ChrF++\"].mean()\n",
    "\n",
    "                timing = self._all_response_times\n",
    "                row[\"Average response time (s)\"] = float(\n",
    "                    timing.query(f\"Model=='{model}' & Run=={run_id}\")[\n",
    "                        \"Average response time (s)\"\n",
    "                    ]\n",
    "                )\n",
    "                summary_rows.append(row)\n",
    "\n",
    "        self._summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    def save_to_excel(self, output_file: Union[str, Path]) -> None:\n",
    "        out = Path(output_file)\n",
    "        with pd.ExcelWriter(out, engine=\"openpyxl\") as writer:\n",
    "            # 1) Summary sheet\n",
    "            self._summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "            # 2) Detailed sheet: pivot so each metric×run is its own column\n",
    "            det = self._detailed_results\n",
    "            det_pivot = det.pivot_table(\n",
    "                index=[\"src\", \"ref\", \"Model\"],\n",
    "                columns=\"Run\",\n",
    "                values=[c for c in [\"BLEU\", \"ChrF\", \"ChrF++\"] if c in det.columns],\n",
    "            )\n",
    "            # flatten MultiIndex → \"BLEU Run 1\", etc.\n",
    "            det_pivot.columns = [\n",
    "                f\"{metric} Run {run}\" for metric, run in det_pivot.columns\n",
    "            ]\n",
    "            det_pivot.reset_index().to_excel(writer, sheet_name=\"Detailed\", index=False)\n",
    "\n",
    "            # 3) Timings sheet: pivot so Run 1/2 are separate columns\n",
    "            t = self._all_response_times\n",
    "            t_pivot = t.pivot(\n",
    "                index=\"Model\",\n",
    "                columns=\"Run\",\n",
    "                values=\"Average response time (s)\"\n",
    "            )\n",
    "            t_pivot.columns = [f\"Run {r}\" for r in t_pivot.columns]\n",
    "            t_pivot.reset_index().to_excel(writer, sheet_name=\"Timings\", index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: python translation_evaluator.py <input.xlsx|csv> <output.xlsx>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    inp, outp = sys.argv[1], sys.argv[2]\n",
    "\n",
    "    # 1) instantiate and load\n",
    "    evaluator = TranslationEvaluator(reference_col=\"en\")\n",
    "    evaluator.load_data(inp)\n",
    "\n",
    "    # 2) pick your model columns:\n",
    "    models = [\n",
    "        \"base_madlad400_translation\",\n",
    "        \"finetuned_madlad400_translation\",\n",
    "        \"finetuned_helsinki_translation\",\n",
    "        \"base_helsinki_translation\"\n",
    "    ]\n",
    "\n",
    "    # 3) run evaluation\n",
    "    evaluator.evaluate(prediction_cols=models)\n",
    "\n",
    "    # 4) write to Excel\n",
    "    evaluator.save_to_excel(outp)\n",
    "    print(f\"Wrote results to {outp}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_evaluator.py\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, output_file: str = \"output_data.xlsx\"):\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        self._data = None\n",
    "\n",
    "        # these will accumulate across runs\n",
    "        self._detailed = pd.DataFrame()\n",
    "        self._summary  = pd.DataFrame()\n",
    "        self._timings  = pd.DataFrame()\n",
    "\n",
    "        self._output_file = Path(output_file)\n",
    "\n",
    "        self._bleu   = BLEU(effective_order=True)      # recommended for sent-level BLEU\n",
    "        self._chrf   = CHRF()\n",
    "        self._chrfpp = CHRF(word_order=2)               # ChrF++\n",
    "\n",
    "    def load_data(self, path: str):\n",
    "        p = Path(path)\n",
    "        if p.suffix in {\".xlsx\", \".xls\"}:\n",
    "            self._data = pd.read_excel(p)\n",
    "        elif p.suffix == \".csv\":\n",
    "            self._data = pd.read_csv(p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type {p.suffix!r}\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: list[str],\n",
    "        reference_col:   str,\n",
    "        metrics:         list[str],\n",
    "        keep_cols:       list[str],\n",
    "        measure_time:    bool,\n",
    "        run_id:          int,\n",
    "    ):\n",
    "        if self._data is None:\n",
    "            raise RuntimeError(\"No data loaded (call load_data first)\")\n",
    "\n",
    "        # verify columns exist\n",
    "        for c in [reference_col] + prediction_cols + keep_cols:\n",
    "            if c not in self._data.columns:\n",
    "                raise KeyError(f\"Column not found: {c}\")\n",
    "\n",
    "        detailed_rows = []\n",
    "        timing_rows   = []\n",
    "\n",
    "        # for each model\n",
    "        for model in prediction_cols:\n",
    "            refs  = self._data[reference_col].astype(str).tolist()\n",
    "            preds = self._data[model].astype(str).tolist()\n",
    "\n",
    "            for idx, (r, p) in enumerate(zip(refs, preds)):\n",
    "                row = {\n",
    "                    reference_col: r,\n",
    "                    \"model\":       model,\n",
    "                    \"run\":         run_id,\n",
    "                }\n",
    "                # carry over any context columns\n",
    "                for kc in keep_cols:\n",
    "                    row[kc] = self._data[kc].iat[idx]\n",
    "\n",
    "                # compute each metric\n",
    "                total_time = 0.0\n",
    "                for m in metrics:\n",
    "                    t0 = time.time()\n",
    "                    if m == \"BLEU\":\n",
    "                        sc = self._bleu.sentence_score(p, [r]).score\n",
    "                    elif m == \"ChrF\":\n",
    "                        sc = self._chrf.sentence_score(p, [r]).score\n",
    "                    elif m == \"ChrF++\":\n",
    "                        sc = self._chrfpp.sentence_score(p, [r]).score\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown metric {m!r}\")\n",
    "                    dt = time.time() - t0\n",
    "\n",
    "                    row[m] = sc\n",
    "                    total_time += dt\n",
    "\n",
    "                if measure_time:\n",
    "                    row[\"response_time\"] = total_time\n",
    "\n",
    "                detailed_rows.append(row)\n",
    "\n",
    "            # after finishing this model/run, record avg response time\n",
    "            if measure_time:\n",
    "                rt = (\n",
    "                    sum(r[\"response_time\"]\n",
    "                        for r in detailed_rows\n",
    "                        if r[\"model\"] == model and r[\"run\"] == run_id)\n",
    "                    / len(preds)\n",
    "                )\n",
    "                timing_rows.append({\n",
    "                    \"model\":            model,\n",
    "                    \"run\":              run_id,\n",
    "                    \"avg_response_time\": rt\n",
    "                })\n",
    "\n",
    "        # append out\n",
    "        self._detailed = pd.concat(\n",
    "            [self._detailed, pd.DataFrame(detailed_rows)],\n",
    "            ignore_index=True,\n",
    "            sort=False\n",
    "        )\n",
    "\n",
    "        # build summary (mean over sentences)\n",
    "        summary_rows = []\n",
    "        for model in prediction_cols:\n",
    "            for m in metrics:\n",
    "                avg = (\n",
    "                    self._detailed\n",
    "                        .loc[\n",
    "                            (self._detailed.model == model) &\n",
    "                            (self._detailed.run == run_id),\n",
    "                            m\n",
    "                        ]\n",
    "                        .mean()\n",
    "                )\n",
    "                summary_rows.append({\n",
    "                    \"model\":  model,\n",
    "                    \"run\":    run_id,\n",
    "                    \"metric\": m,\n",
    "                    \"value\":  avg\n",
    "                })\n",
    "            if measure_time:\n",
    "                avg_rt = next(\n",
    "                    x[\"avg_response_time\"]\n",
    "                    for x in timing_rows\n",
    "                    if x[\"model\"] == model and x[\"run\"] == run_id\n",
    "                )\n",
    "                summary_rows.append({\n",
    "                    \"model\":  model,\n",
    "                    \"run\":    run_id,\n",
    "                    \"metric\": \"avg_response_time\",\n",
    "                    \"value\":  avg_rt\n",
    "                })\n",
    "\n",
    "        self._summary = pd.concat(\n",
    "            [self._summary, pd.DataFrame(summary_rows)],\n",
    "            ignore_index=True,\n",
    "            sort=False\n",
    "        )\n",
    "\n",
    "        if measure_time:\n",
    "            self._timings = pd.concat(\n",
    "                [self._timings, pd.DataFrame(timing_rows)],\n",
    "                ignore_index=True,\n",
    "                sort=False\n",
    "            )\n",
    "\n",
    "    def save_excel(self):\n",
    "        with pd.ExcelWriter(self._output_file, engine=\"openpyxl\") as w:\n",
    "            # ─── Summary sheet ────────────────────────────────────────────────\n",
    "            sum_pivot = (\n",
    "                self._summary\n",
    "                    .pivot(index=\"model\", columns=[\"run\",\"metric\"], values=\"value\")\n",
    "                    .sort_index(axis=1, level=0)\n",
    "            )\n",
    "            sum_pivot.to_excel(w, sheet_name=\"Summary\")\n",
    "\n",
    "            # ─── Detailed sheet ───────────────────────────────────────────────\n",
    "            # we pivot on: keep_cols + [reference_col] as index, columns=(model,run)\n",
    "            # values = all metrics, plus response_time if present\n",
    "            all_values = [c for c in self._detailed.columns\n",
    "                          if c not in (\"model\",\"run\") + tuple(keep_cols) + (reference_col,)]\n",
    "            det_pivot = (\n",
    "                self._detailed\n",
    "                    .pivot_table(\n",
    "                        index = keep_cols + [reference_col],\n",
    "                        columns = [\"model\",\"run\"],\n",
    "                        values = all_values,\n",
    "                        aggfunc = \"first\"   # each (sentence,model,run) is unique\n",
    "                    )\n",
    "            )\n",
    "            det_pivot.to_excel(w, sheet_name=\"Detailed\")\n",
    "\n",
    "            # ─── Timings sheet ────────────────────────────────────────────────\n",
    "            if not self._timings.empty:\n",
    "                t_pivot = (\n",
    "                    self._timings\n",
    "                        .pivot(index=\"model\", columns=\"run\", values=\"avg_response_time\")\n",
    "                )\n",
    "                t_pivot.to_excel(w, sheet_name=\"Timings\")\n",
    "\n",
    "        print(f\"✅  Wrote all three sheets to {self._output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_eval.py\n",
    "\n",
    "from translation_evaluator import TranslationEvaluator\n",
    "\n",
    "def main():\n",
    "    te = TranslationEvaluator(output_file=\"output_data.xlsx\")\n",
    "    te.load_data(\"input_data.xlsx\")\n",
    "\n",
    "    MODELS  = [\n",
    "        \"base_madlad400_translation\",\n",
    "        \"finetuned_madlad400_translation\",\n",
    "        \"finetuned_helsinki_translation\",\n",
    "        \"base_helsinki_translation\",\n",
    "    ]\n",
    "    METRICS = [\"BLEU\",\"ChrF\",\"ChrF++\"]\n",
    "    KEEP    = [\"es\"]\n",
    "    REF     = \"en\"\n",
    "\n",
    "    # Run #1: with timing\n",
    "    te.evaluate(\n",
    "        prediction_cols=MODELS,\n",
    "        reference_col=REF,\n",
    "        metrics=METRICS,\n",
    "        keep_cols=KEEP,\n",
    "        measure_time=True,\n",
    "        run_id=1,\n",
    "    )\n",
    "\n",
    "    # Run #2: metrics only\n",
    "    te.evaluate(\n",
    "        prediction_cols=MODELS,\n",
    "        reference_col=REF,\n",
    "        metrics=METRICS,\n",
    "        keep_cols=KEEP,\n",
    "        measure_time=False,\n",
    "        run_id=2,\n",
    "    )\n",
    "\n",
    "    te.save_excel()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, output_file: str = \"output_data.xlsx\"):\n",
    "        self.output_file = Path(output_file)\n",
    "        # Will hold the raw data + all per‐sentence metrics\n",
    "        self._data: pd.DataFrame = pd.DataFrame()\n",
    "        self._detailed: pd.DataFrame = pd.DataFrame()\n",
    "        # Will accumulate model‐level metrics (one row per model×run)\n",
    "        self._model_metrics = pd.DataFrame(\n",
    "            columns=[\"model\", \"run\", \"BLEU\", \"CHRF\", \"ChrF++\"]\n",
    "        )\n",
    "        # Will accumulate average response times\n",
    "        self._all_response_times = pd.DataFrame(\n",
    "            columns=[\"model\", \"run\", \"avg_response_time\"]\n",
    "        )\n",
    "\n",
    "    def load_data(self, input_path: str) -> None:\n",
    "        \"\"\"Load your input file. Must have at least: en, es, plus one column per model.\"\"\"\n",
    "        p = Path(input_path)\n",
    "        if p.suffix in (\".xlsx\", \".xls\"):\n",
    "            self._data = pd.read_excel(p)\n",
    "        elif p.suffix == \".csv\":\n",
    "            self._data = pd.read_csv(p)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type: \" + str(p.suffix))\n",
    "\n",
    "        if \"en\" not in self._data.columns or \"es\" not in self._data.columns:\n",
    "            raise ValueError(\"Input must have 'en' and 'es' columns.\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: list[str],\n",
    "        reference_col: str = \"en\",\n",
    "        metrics: list[str] = [\"BLEU\", \"CHRF\", \"ChrF++\"],\n",
    "        keep_cols: list[str] = [\"es\"],\n",
    "        measure_time: bool = True,\n",
    "        run_id: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Compute per‐sentence and corpus‐level metrics, optionally timing run.\n",
    "        - prediction_cols: list of model‐output column names\n",
    "        - reference_col: name of the reference column (default \"en\")\n",
    "        - keep_cols: any other columns to carry through into the Detailed sheet\n",
    "        - measure_time: if True, record per‐sentence timings\n",
    "        - run_id: an integer tag for this pass (1 or 2)\n",
    "        \"\"\"\n",
    "        # Store for later pivoting\n",
    "        self._prediction_cols = prediction_cols\n",
    "        self._reference_col = reference_col\n",
    "        self._keep_cols = keep_cols\n",
    "        self._metrics = metrics\n",
    "\n",
    "        # Validate\n",
    "        missing = {reference_col} | set(prediction_cols) | set(keep_cols) \\\n",
    "                  - set(self._data.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns in data: {missing}\")\n",
    "\n",
    "        # Prepare a little detail‐frame\n",
    "        detail_rows = []\n",
    "        for model in prediction_cols:\n",
    "            refs = self._data[reference_col].astype(str).tolist()\n",
    "            hyps = self._data[model].astype(str).tolist()\n",
    "\n",
    "            # sentence‐level CHRF\n",
    "            chrf_fn = CHRF(word_order=2).sentence_score\n",
    "\n",
    "            timings = []\n",
    "            scores = {m: [] for m in metrics}\n",
    "\n",
    "            for ref, hyp in zip(refs, hyps):\n",
    "                start = time.perf_counter()\n",
    "                # BLEU\n",
    "                if \"BLEU\" in metrics:\n",
    "                    bleu = sentence_bleu(hyp, [ref]).score\n",
    "                    scores[\"BLEU\"].append(bleu)\n",
    "                # CHRF\n",
    "                if \"CHRF\" in metrics:\n",
    "                    c = chrf_fn(hyp, [ref]).score\n",
    "                    scores[\"CHRF\"].append(c)\n",
    "                # ChrF++\n",
    "                if \"ChrF++\" in metrics:\n",
    "                    # sacrebleu.CHRF with word_order=2 defaults to ChrF++\n",
    "                    chfpp = chrf_fn(hyp, [ref]).score\n",
    "                    scores[\"ChrF++\"].append(chfpp)\n",
    "\n",
    "                elapsed = time.perf_counter() - start\n",
    "                timings.append(elapsed if measure_time else 0.0)\n",
    "\n",
    "            # Build detail‐rows for this model\n",
    "            for idx in range(len(refs)):\n",
    "                row = {\n",
    "                    \"model\": model,\n",
    "                    \"run\": f\"Run {run_id}\",\n",
    "                    reference_col: refs[idx],\n",
    "                }\n",
    "                # keep columns\n",
    "                for col in keep_cols:\n",
    "                    row[col] = self._data.loc[idx, col]\n",
    "                # metrics\n",
    "                for m in metrics:\n",
    "                    row[m] = scores[m][idx]\n",
    "                # timing\n",
    "                row[\"response_time\"] = timings[idx]\n",
    "                detail_rows.append(row)\n",
    "\n",
    "            # corpus‐level aggregates\n",
    "            agg = {\n",
    "                \"model\": model,\n",
    "                \"run\": f\"Run {run_id}\",\n",
    "                \"BLEU\": sum(scores[\"BLEU\"]) / len(scores[\"BLEU\"]) if \"BLEU\" in metrics else None,\n",
    "                \"CHRF\": sum(scores[\"CHRF\"]) / len(scores[\"CHRF\"]) if \"CHRF\" in metrics else None,\n",
    "                \"ChrF++\": sum(scores[\"ChrF++\"]) / len(scores[\"ChrF++\"]) if \"ChrF++\" in metrics else None,\n",
    "            }\n",
    "            self._model_metrics = pd.concat(\n",
    "                [self._model_metrics, pd.DataFrame([agg])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "            # average timing\n",
    "            if measure_time:\n",
    "                avg_t = sum(timings) / len(timings)\n",
    "            else:\n",
    "                avg_t = 0.0\n",
    "            self._all_response_times = pd.concat(\n",
    "                [\n",
    "                    self._all_response_times,\n",
    "                    pd.DataFrame([{\n",
    "                        \"model\": model,\n",
    "                        \"run\": f\"Run {run_id}\",\n",
    "                        \"avg_response_time\": avg_t,\n",
    "                    }])\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "        # stitch together a single detailed DataFrame\n",
    "        self._detailed = pd.DataFrame(detail_rows)\n",
    "\n",
    "    def save_excel(self):\n",
    "        with pd.ExcelWriter(self.output_file, engine=\"openpyxl\") as writer:\n",
    "            # ─── Detailed sheet ─────────────────────────────────────────\n",
    "            idx = self._keep_cols + [self._reference_col]\n",
    "            vals = self._metrics + [\"response_time\"]\n",
    "            det_pivot = (\n",
    "                self._detailed\n",
    "                    .pivot_table(\n",
    "                        index=idx,\n",
    "                        columns=[\"model\", \"run\"],\n",
    "                        values=vals,\n",
    "                        aggfunc=\"first\",\n",
    "                    )\n",
    "            )\n",
    "            det_pivot.to_excel(writer, sheet_name=\"Detailed\")\n",
    "\n",
    "            # ─── Summary sheet ──────────────────────────────────────────\n",
    "            sum_pivot = (\n",
    "                self._model_metrics\n",
    "                    .pivot_table(\n",
    "                        index=\"model\",\n",
    "                        columns=\"run\",\n",
    "                        values=self._metrics,\n",
    "                        aggfunc=\"first\",\n",
    "                    )\n",
    "            )\n",
    "            sum_pivot.to_excel(writer, sheet_name=\"Summary\")\n",
    "\n",
    "            # ─── Timings sheet ─────────────────────────────────────────\n",
    "            time_pivot = (\n",
    "                self._all_response_times\n",
    "                    .pivot_table(\n",
    "                        index=\"model\",\n",
    "                        columns=\"run\",\n",
    "                        values=\"avg_response_time\",\n",
    "                        aggfunc=\"first\",\n",
    "                    )\n",
    "            )\n",
    "            time_pivot.to_excel(writer, sheet_name=\"Timings\")\n",
    "\n",
    "        print(f\"Wrote all three sheets to {self.output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    te = TranslationEvaluator(\"output_data.xlsx\")\n",
    "    te.load_data(\"input_data.xlsx\")\n",
    "\n",
    "    # Run 1: measure per‐sentence timing\n",
    "    te.evaluate(\n",
    "        prediction_cols=[\n",
    "            \"base_madlad400_translation\",\n",
    "            \"finetuned_madlad400_translation\",\n",
    "            \"finetuned_helsinki_translation\",\n",
    "            \"base_helsinki_translation\",\n",
    "        ],\n",
    "        reference_col=\"en\",\n",
    "        metrics=[\"BLEU\", \"CHRF\", \"ChrF++\"],\n",
    "        keep_cols=[\"es\"],\n",
    "        measure_time=True,\n",
    "        run_id=1,\n",
    "    )\n",
    "\n",
    "    # Run 2: no timing\n",
    "    te.evaluate(\n",
    "        prediction_cols=[\n",
    "            \"base_madlad400_translation\",\n",
    "            \"finetuned_madlad400_translation\",\n",
    "            \"finetuned_helsinki_translation\",\n",
    "            \"base_helsinki_translation\",\n",
    "        ],\n",
    "        reference_col=\"en\",\n",
    "        metrics=[\"BLEU\", \"CHRF\", \"ChrF++\"],\n",
    "        keep_cols=[\"es\"],\n",
    "        measure_time=False,\n",
    "        run_id=2,\n",
    "    )\n",
    "\n",
    "    te.save_excel()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Translation_evaluator_Research import TranslationEvaluator\n",
    "\n",
    "def main():\n",
    "    te = TranslationEvaluator(output_file=\"output_data.xlsx\")\n",
    "\n",
    "    # 1) load exactly the right sheet\n",
    "    te.load_data(\"input_data.xlsx\")  \n",
    "    # If your sheet isn’t the first one, change the loader inside the class as above.\n",
    "\n",
    "    # debug print—make sure you really have the six columns you expect:\n",
    "    print(\"Loaded columns:\", te._data.columns.tolist())\n",
    "\n",
    "    models = [\n",
    "        \"base_madlad400_translation\",\n",
    "        \"finetuned_madlad400_translation\",\n",
    "        \"finetuned_helsinki_translation\",\n",
    "        \"base_helsinki_translation\"\n",
    "    ]\n",
    "    keep = [\"es\"]\n",
    "    metrics = [\"BLEU\", \"ChrF\", \"ChrF++\"]\n",
    "\n",
    "    # Run 1: measure timing\n",
    "    te.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=\"en\",\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=1,\n",
    "        measure_time=True,\n",
    "    )\n",
    "\n",
    "    # Run 2: no timing\n",
    "    te.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=\"en\",\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=2,\n",
    "        measure_time=False,\n",
    "    )\n",
    "\n",
    "    # write everything out\n",
    "    te.save_excel()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
