{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_evaluator.py\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, sentence_chrf2\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    - load_data(): CSV or Excel\n",
    "    - evaluate(): computes per-model & per-metric columns (plus optional per-model timing)\n",
    "    - get_detailed_results(): DataFrame with those columns\n",
    "    - get_model_metrics(): dict of all metric means\n",
    "    - get_response_times(): DataFrame of run_id vs avg_response_time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data: pd.DataFrame | None = None\n",
    "        self._detailed_results: pd.DataFrame = pd.DataFrame()\n",
    "        self._model_metrics: dict[str, float] = {}\n",
    "        self._response_times: pd.DataFrame = pd.DataFrame(\n",
    "            [], columns=[\"run_id\", \"avg_response_time\"]\n",
    "        )\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        p = Path(file_path)\n",
    "        if p.suffix.lower() == \".csv\":\n",
    "            self._data = pd.read_csv(p)\n",
    "        elif p.suffix.lower() in (\".xls\", \".xlsx\"):\n",
    "            self._data = pd.read_excel(p)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format (must be .csv, .xls or .xlsx)\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col:    str,\n",
    "        metrics:          List[str],\n",
    "        keep_cols:        List[str],\n",
    "        run_id:           int | None = None,\n",
    "        measure_time:     bool      = False,\n",
    "    ) -> None:\n",
    "        if self._data is None:\n",
    "            raise RuntimeError(\"No data loaded; call load_data() first.\")\n",
    "\n",
    "        rows = []\n",
    "        time_records: list[float] = []\n",
    "\n",
    "        for _, r in self._data.iterrows():\n",
    "            rec = {c: r[c] for c in keep_cols}\n",
    "            ref = str(r[reference_col])\n",
    "\n",
    "            for col in prediction_cols:\n",
    "                pred = str(r[col])\n",
    "                if measure_time:\n",
    "                    start = time.time()\n",
    "\n",
    "                # compute each metric and store under \"{col} {METRIC}\"\n",
    "                for m in metrics:\n",
    "                    m_up = m.upper()\n",
    "                    if m_up == \"BLEU\":\n",
    "                        sc = sentence_bleu(pred, [ref]).score\n",
    "                    elif m_up == \"CHRF\":\n",
    "                        sc = sentence_chrf(pred, [ref]).score\n",
    "                    elif m_up in (\"CHRF++\", \"CHRFF++\", \"CHR F++\"):\n",
    "                        sc = sentence_chrf2(pred, [ref]).score\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported metric: {m!r}\")\n",
    "                    rec[f\"{col} {m_up}\"] = sc\n",
    "\n",
    "                # optional timing column per model\n",
    "                if measure_time and run_id is not None:\n",
    "                    elapsed = time.time() - start\n",
    "                    rec[f\"{col} Response Time Run {run_id}\"] = elapsed\n",
    "                    time_records.append(elapsed)\n",
    "\n",
    "            rows.append(rec)\n",
    "\n",
    "        # build detailed-results DataFrame\n",
    "        self._detailed_results = pd.DataFrame(rows)\n",
    "\n",
    "        # aggregate model metrics: mean of every \"{col} {METRIC}\"\n",
    "        metric_cols = [f\"{c} {m.upper()}\" for c in prediction_cols for m in metrics]\n",
    "        self._model_metrics = {\n",
    "            col: float(self._detailed_results[col].mean()) for col in metric_cols\n",
    "        }\n",
    "\n",
    "        # if timing was measured, record the run average\n",
    "        if measure_time and run_id is not None:\n",
    "            avg = float(np.mean(time_records))\n",
    "            new = pd.DataFrame([{\"run_id\": run_id, \"avg_response_time\": avg}])\n",
    "            self._response_times = pd.concat(\n",
    "                [self._response_times, new], ignore_index=True\n",
    "            )\n",
    "\n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        return self._detailed_results.copy()\n",
    "\n",
    "    def get_model_metrics(self) -> dict[str, float]:\n",
    "        return dict(self._model_metrics)\n",
    "\n",
    "    def get_response_times(self) -> pd.DataFrame:\n",
    "        return self._response_times.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation_evaluator import TranslationEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    evaluator = TranslationEvaluator()\n",
    "    evaluator.load_data(\"input_data.xlsx\")\n",
    "\n",
    "    models   = ['base_madlad400_translation', 'finetuned_madlad400_translation']\n",
    "    keep     = [\"en\", \"es\"]\n",
    "    ref_col  = \"en\"\n",
    "    metrics  = [\"BLEU\", \"ChrF\", \"ChrF++\"]\n",
    "\n",
    "    # Run #1, with timing\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=1,\n",
    "        measure_time=True\n",
    "    )\n",
    "\n",
    "    # Run #2, without timing\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=2,\n",
    "        measure_time=False\n",
    "    )\n",
    "\n",
    "    # View detailed results\n",
    "    df = evaluator.get_detailed_results()\n",
    "    print(\"Columns in detailed results:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Example: select exactly the columns you need:\n",
    "    want = [\n",
    "      'base_madlad400_translation BLEU',\n",
    "      'base_madlad400_translation CHRF',\n",
    "      'base_madlad400_translation Response Time Run 1',\n",
    "      # etc. for the rest...\n",
    "    ]\n",
    "    print(df[want].head(), \"\\n\")\n",
    "\n",
    "    # View aggregated metrics\n",
    "    print(\"Aggregated metrics:\")\n",
    "    print(pd.DataFrame([evaluator.get_model_metrics()]), \"\\n\")\n",
    "\n",
    "    # View timing table\n",
    "    print(\"Response times by run:\")\n",
    "    print(evaluator.get_response_times())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation_evaluator_Research.py\n",
    "\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, corpus_bleu, sentence_chrf, corpus_chrf\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, output_file: Union[str, Path] = \"output_data.xlsx\"):\n",
    "        \"\"\"\n",
    "        :param output_file: where to write the Excel report\n",
    "        \"\"\"\n",
    "        self._data: Optional[pd.DataFrame] = None\n",
    "        self._detailed_results: Optional[pd.DataFrame] = None\n",
    "        self._model_metrics: Optional[pd.DataFrame] = None\n",
    "        self._output_file = Path(output_file)\n",
    "\n",
    "        # suppress any tokenizer warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load your input_data.xlsx / .csv into a DataFrame.\n",
    "        Expects at least one column for references (e.g. \"en\") and\n",
    "        one or more columns of model outputs.\n",
    "        \"\"\"\n",
    "        p = Path(file_path)\n",
    "        if p.suffix.lower() in {\".xls\", \".xlsx\"}:\n",
    "            df = pd.read_excel(p)\n",
    "        elif p.suffix.lower() == \".csv\":\n",
    "            df = pd.read_csv(p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported extension: {p.suffix}\")\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Loaded DataFrame is empty.\")\n",
    "        self._data = df\n",
    "        return df\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col: str = \"en\",\n",
    "        metrics: List[str] = [\"BLEU\", \"CHRF\", \"ChrF++\"],\n",
    "        keep_cols: Optional[List[str]] = None,\n",
    "        measure_time: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Compute per-sentence and corpus-level BLEU, CHRF and ChrF++.\n",
    "        :param prediction_cols: list of your model-output column names\n",
    "        :param reference_col: the column with your human/reference translation\n",
    "        :param metrics: choose any of \"BLEU\", \"CHRF\", \"ChrF++\"\n",
    "        :param keep_cols: any additional columns (e.g. \"es\") you want carried into the Detailed sheet\n",
    "        :param measure_time: if True, also record per-sentence latency\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            raise ValueError(\"Please call load_data() first.\")\n",
    "\n",
    "        df = self._data.copy()\n",
    "        keep_cols = keep_cols or []\n",
    "\n",
    "        # sanity‐check that all required columns exist\n",
    "        for col in [reference_col] + prediction_cols + keep_cols:\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"Column not found: {col}\")\n",
    "\n",
    "        detailed_records = []\n",
    "        # 1) per-sentence scores (+ optional timing)\n",
    "        for model in prediction_cols:\n",
    "            for _, row in df.iterrows():\n",
    "                hyp = str(row[model])\n",
    "                ref = str(row[reference_col])\n",
    "                rec = {\"model\": model}\n",
    "                if measure_time:\n",
    "                    t0 = time.time()\n",
    "\n",
    "                if \"BLEU\" in metrics:\n",
    "                    rec[\"BLEU\"] = sentence_bleu(hyp, [ref]).score\n",
    "                if \"CHRF\" in metrics:\n",
    "                    rec[\"CHRF\"] = sentence_chrf(hyp, [ref]).score\n",
    "                if \"ChrF++\" in metrics:\n",
    "                    # ChrF++ = word_order=2\n",
    "                    rec[\"ChrF++\"] = sentence_chrf(hyp, [ref], word_order=2).score\n",
    "\n",
    "                if measure_time:\n",
    "                    rec[\"response_time\"] = time.time() - t0\n",
    "\n",
    "                # carry over any additional columns (e.g. \"es\")\n",
    "                for col in keep_cols:\n",
    "                    rec[col] = row[col]\n",
    "\n",
    "                # also include the reference text if you like\n",
    "                rec[reference_col] = ref\n",
    "                detailed_records.append(rec)\n",
    "\n",
    "        self._detailed_results = pd.DataFrame(detailed_records)\n",
    "\n",
    "        # 2) corpus-level (aggregate) metrics\n",
    "        summary = []\n",
    "        for model in prediction_cols:\n",
    "            hyps = df[model].astype(str).tolist()\n",
    "            refs = df[reference_col].astype(str).tolist()\n",
    "            rec = {\"model\": model}\n",
    "            if \"BLEU\" in metrics:\n",
    "                rec[\"BLEU\"] = corpus_bleu(hyps, [refs]).score\n",
    "            if \"CHRF\" in metrics:\n",
    "                rec[\"CHRF\"] = corpus_chrf(hyps, [refs]).score\n",
    "            if \"ChrF++\" in metrics:\n",
    "                rec[\"ChrF++\"] = corpus_chrf(hyps, [refs], word_order=2).score\n",
    "            if measure_time:\n",
    "                times = self._detailed_results.query(\"model == @model\")[\"response_time\"]\n",
    "                rec[\"avg_response_time\"] = times.mean()\n",
    "            summary.append(rec)\n",
    "\n",
    "        self._model_metrics = pd.DataFrame(summary)\n",
    "\n",
    "        # 3) write them both to an Excel file\n",
    "        with pd.ExcelWriter(self._output_file) as writer:\n",
    "            self._detailed_results.to_excel(\n",
    "                writer, sheet_name=\"Detailed\", index=False\n",
    "            )\n",
    "            self._model_metrics.to_excel(\n",
    "                writer, sheet_name=\"Summary\", index=False\n",
    "            )\n",
    "\n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        if self._detailed_results is None:\n",
    "            raise ValueError(\"No detailed results – did you call evaluate()?\")\n",
    "        return self._detailed_results\n",
    "\n",
    "    def get_model_metrics(self) -> pd.DataFrame:\n",
    "        if self._model_metrics is None:\n",
    "            raise ValueError(\"No model metrics – did you call evaluate()?\")\n",
    "        return self._model_metrics\n",
    "\n",
    "    def get_response_times(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Only valid if you passed measure_time=True to evaluate().\n",
    "        \"\"\"\n",
    "        if self._detailed_results is None or \"response_time\" not in self._detailed_results:\n",
    "            raise ValueError(\"No timing data – did you evaluate(measure_time=True)?\")\n",
    "        return self._detailed_results[[\"model\", \"response_time\"]]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- example usage ---\n",
    "    evaluator = TranslationEvaluator(output_file=\"bert_translations_report.xlsx\")\n",
    "    evaluator.load_data(\"input_data.xlsx\")\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=[\n",
    "            \"base_madlad400_translation\",\n",
    "            \"finetuned_madlad400_translation\",\n",
    "            \"finetuned_helsinki_translation\",\n",
    "            \"base_helsinki_translation\",\n",
    "        ],\n",
    "        reference_col=\"en\",\n",
    "        metrics=[\"BLEU\", \"CHRF\", \"ChrF++\"],\n",
    "        keep_cols=[\"es\"],\n",
    "        measure_time=True,      # set False if you don't need per-sentence timing\n",
    "    )\n",
    "\n",
    "    print(\"=== Detailed results (first 5 rows) ===\")\n",
    "    print(evaluator.get_detailed_results().head(), \"\\n\")\n",
    "\n",
    "    print(\"=== Summary (corpus-level) ===\")\n",
    "    print(evaluator.get_model_metrics(), \"\\n\")\n",
    "\n",
    "    print(\"=== Response times ===\")\n",
    "    print(evaluator.get_response_times().head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
