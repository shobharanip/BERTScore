{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing translation_evaluator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile translation_evaluator.py\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, corpus_bleu, corpus_chrf\n",
    "from typing import List, Optional, Union\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    A complete translation evaluation system that handles:\n",
    "    - Loading data from files\n",
    "    - Calculating metrics (BLEU, CHRF)\n",
    "    - Generating detailed and summary reports\n",
    "    - Exporting to Excel with professional formatting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        warnings.simplefilter('ignore')  # Suppress sacrebleu warnings\n",
    "        self._data = None\n",
    "        self._detailed_results = None\n",
    "        self._model_metrics = None\n",
    "    \n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        \"\"\"Load translation data from file (CSV or Excel)\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        if file_path.suffix == '.csv':\n",
    "            self._data = pd.read_csv(file_path)\n",
    "        elif file_path.suffix in ('.xlsx', '.xls'):\n",
    "            self._data = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col: str = \"English\", \n",
    "        metrics: List[str] = [\"BLEU\", \"CHRF\"],\n",
    "        keep_cols: Optional[List[str]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run complete evaluation pipeline\n",
    "        \n",
    "        Args:\n",
    "            prediction_cols: List of prediction columns to evaluate\n",
    "            reference_col: Reference translation column\n",
    "            metrics: List of metrics to compute\n",
    "            keep_cols: Additional columns to preserve in output\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        if keep_cols is None:\n",
    "            keep_cols = []\n",
    "        \n",
    "        # Validate columns exist\n",
    "        self._validate_columns(reference_col, prediction_cols, keep_cols)\n",
    "        \n",
    "        # 1. Compute detailed metrics\n",
    "        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep_cols)\n",
    "        \n",
    "        # 2. Compute model metrics\n",
    "        self._compute_model_metrics(reference_col, prediction_cols, metrics)\n",
    "        \n",
    "        # 3. Generate report\n",
    "        self._generate_report()\n",
    "    \n",
    "    def _validate_columns(self, reference_col: str, prediction_cols: List[str], keep_cols: List[str]) -> None:\n",
    "        \"\"\"Validate all required columns exist\"\"\"\n",
    "        missing = [col for col in [reference_col] + prediction_cols + keep_cols \n",
    "                  if col not in self._data.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns not found in data: {missing}\")\n",
    "    \n",
    "    def _compute_detailed_metrics(self, reference_col: str, prediction_cols: List[str], \n",
    "                                metrics: List[str], keep_cols: List[str]) -> None:\n",
    "        \"\"\"Calculate per-row metrics for all predictions\"\"\"\n",
    "        self._detailed_results = self._data.copy()\n",
    "        \n",
    "        for pred_col in prediction_cols:\n",
    "            if \"BLEU\" in metrics:\n",
    "                self._detailed_results[f\"{pred_col} BLEU\"] = self._detailed_results.apply(\n",
    "                    lambda row: sentence_bleu(row[pred_col], [row[reference_col]]).score,\n",
    "                    axis=1\n",
    "                )\n",
    "            if \"CHRF\" in metrics:\n",
    "                self._detailed_results[f\"{pred_col} CHRF\"] = self._detailed_results.apply(\n",
    "                    lambda row: sentence_chrf(row[pred_col], [row[reference_col]]).score,\n",
    "                    axis=1\n",
    "                )\n",
    "        \n",
    "        # Reorder columns: keep_cols + (each prediction with its metrics)\n",
    "        new_columns = keep_cols.copy()\n",
    "        for pred_col in prediction_cols:\n",
    "            new_columns.append(pred_col)\n",
    "            if \"BLEU\" in metrics:\n",
    "                new_columns.append(f\"{pred_col} BLEU\")\n",
    "            if \"CHRF\" in metrics:\n",
    "                new_columns.append(f\"{pred_col} CHRF\")\n",
    "        \n",
    "        self._detailed_results = self._detailed_results[new_columns]\n",
    "    \n",
    "    def _compute_model_metrics(self, reference_col: str, prediction_cols: List[str], \n",
    "                             metrics: List[str]) -> None:\n",
    "        \"\"\"Calculate aggregate metrics for each model\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for pred_col in prediction_cols:\n",
    "            references = self._data[reference_col].tolist()\n",
    "            predictions = self._data[pred_col].tolist()\n",
    "            \n",
    "            model_results = {\"Model\": pred_col}\n",
    "            if \"BLEU\" in metrics:\n",
    "                model_results[\"BLEU\"] = corpus_bleu(predictions, [references]).score\n",
    "            if \"CHRF\" in metrics:\n",
    "                model_results[\"CHRF\"] = corpus_chrf(predictions, [references]).score\n",
    "            \n",
    "            results.append(model_results)\n",
    "        \n",
    "        self._model_metrics = pd.DataFrame(results)\n",
    "    \n",
    "    def _generate_report(self) -> None:\n",
    "        \"\"\"Generate Excel report with formatted output\"\"\"\n",
    "        with pd.ExcelWriter(\"translation_results.xlsx\", engine='xlsxwriter') as writer:\n",
    "            # Detailed Results sheet\n",
    "            self._detailed_results.to_excel(writer, sheet_name=\"Detailed Results\", index=False)\n",
    "            \n",
    "            # Model Metrics sheet with formatted table\n",
    "            self._model_metrics.to_excel(writer, sheet_name=\"Model Metrics\", index=False)\n",
    "            \n",
    "            # Formatting\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets[\"Model Metrics\"]\n",
    "            \n",
    "            # Add formatted table\n",
    "            (max_row, max_col) = self._model_metrics.shape\n",
    "            column_settings = [{'header': col} for col in self._model_metrics.columns]\n",
    "            worksheet.add_table(0, 0, max_row, max_col-1, {\n",
    "                'columns': column_settings,\n",
    "                'style': 'Table Style Medium 9',\n",
    "                'name': 'ModelMetrics'\n",
    "            })\n",
    "            \n",
    "            # Auto-adjust columns\n",
    "            for i, col in enumerate(self._model_metrics.columns):\n",
    "                max_len = max(self._model_metrics[col].astype(str).map(len).max(), len(col))\n",
    "                worksheet.set_column(i, i, max_len + 2)\n",
    "    \n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the detailed results DataFrame\"\"\"\n",
    "        return self._detailed_results.copy()\n",
    "    \n",
    "    def get_model_metrics(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the model metrics DataFrame\"\"\"\n",
    "        return self._model_metrics.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting translation_evaluator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile translation_evaluator.py\n",
    "import time\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, corpus_bleu, corpus_chrf\n",
    "from typing import List, Optional, Union\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    A translation evaluation system that handles:\n",
    "    - Loading data\n",
    "    - Computing BLEU/CHRF per row\n",
    "    - Tracking response time per row\n",
    "    - Aggregating model metrics and average response times\n",
    "    - Exporting to Excel with three sheets:\n",
    "      'Detailed Results', 'Model Metrics', 'Response Times'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        warnings.simplefilter('ignore')\n",
    "        self._data = None\n",
    "        self._detailed_results = None\n",
    "        self._model_metrics = None\n",
    "        # Accumulate across runs\n",
    "        self._all_response_times = pd.DataFrame([], columns=[\"Model\", \"Run\", \"Average Response Time\"])\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        file_path = Path(file_path)\n",
    "        if file_path.suffix == '.csv':\n",
    "            self._data = pd.read_csv(file_path)\n",
    "        elif file_path.suffix in ('.xlsx', '.xls'):\n",
    "            self._data = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col: str = \"English\",\n",
    "        metrics: List[str] = [\"BLEU\", \"CHRF\"],\n",
    "        keep_cols: Optional[List[str]] = None,\n",
    "        run_id: int = 1\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run a full evaluation pass.\n",
    "        - prediction_cols: names of model output columns\n",
    "        - reference_col: name of the reference column\n",
    "        - metrics: which metrics to compute\n",
    "        - keep_cols: extra columns to carry through to detailed results\n",
    "        - run_id: an integer label for this run (e.g. 1 or 2)\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        keep_cols = keep_cols or []\n",
    "        self._validate_columns(reference_col, prediction_cols, keep_cols)\n",
    "\n",
    "        # 1. Detailed per-row metrics + response times\n",
    "        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep_cols)\n",
    "        # 2. Aggregate BLEU/CHRF\n",
    "        self._compute_model_metrics(reference_col, prediction_cols, metrics)\n",
    "        # 3. Record avg response times for this run\n",
    "        self._record_response_times(prediction_cols, run_id)\n",
    "        # 4. Write out all three sheets\n",
    "        self._generate_report()\n",
    "\n",
    "    def _validate_columns(self, reference_col, prediction_cols, keep_cols):\n",
    "        missing = [col for col in [reference_col] + prediction_cols + keep_cols\n",
    "                   if col not in self._data.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns not found in data: {missing}\")\n",
    "\n",
    "    def _compute_detailed_metrics(\n",
    "        self, reference_col: str, prediction_cols: List[str],\n",
    "        metrics: List[str], keep_cols: List[str]\n",
    "    ) -> None:\n",
    "        df = self._data.copy()\n",
    "        # Prepare storage for response times per model\n",
    "        response_time_cols = {m: [] for m in prediction_cols}\n",
    "\n",
    "        # Compute metrics + timing\n",
    "        for m in prediction_cols:\n",
    "            # initialize empty columns\n",
    "            df[f\"{m} BLEU\"] = None\n",
    "            df[f\"{m} CHRF\"] = None\n",
    "            for idx, row in df.iterrows():\n",
    "                start = time.time()\n",
    "                if \"BLEU\" in metrics:\n",
    "                    df.at[idx, f\"{m} BLEU\"] = sentence_bleu(row[m], [row[reference_col]]).score\n",
    "                if \"CHRF\" in metrics:\n",
    "                    df.at[idx, f\"{m} CHRF\"] = sentence_chrf(row[m], [row[reference_col]]).score\n",
    "                end = time.time()\n",
    "                response_time_cols[m].append(end - start)\n",
    "\n",
    "            # attach per-row times\n",
    "            df[f\"{m} Response Time\"] = response_time_cols[m]\n",
    "\n",
    "        # Reorder: keep_cols + for each model: [model, model BLEU, model CHRF, model Response Time]\n",
    "        columns = []\n",
    "        for c in keep_cols:\n",
    "            columns.append(c)\n",
    "        for m in prediction_cols:\n",
    "            columns += [m]\n",
    "            if \"BLEU\" in metrics:\n",
    "                columns.append(f\"{m} BLEU\")\n",
    "            if \"CHRF\" in metrics:\n",
    "                columns.append(f\"{m} CHRF\")\n",
    "            columns.append(f\"{m} Response Time\")\n",
    "\n",
    "        self._detailed_results = df[columns]\n",
    "\n",
    "    def _compute_model_metrics(\n",
    "        self, reference_col: str, prediction_cols: List[str],\n",
    "        metrics: List[str]\n",
    "    ) -> None:\n",
    "        rows = []\n",
    "        for m in prediction_cols:\n",
    "            refs = self._data[reference_col].tolist()\n",
    "            hyps = self._data[m].tolist()\n",
    "            res = {\"Model\": m}\n",
    "            if \"BLEU\" in metrics:\n",
    "                res[\"BLEU\"] = corpus_bleu(hyps, [refs]).score\n",
    "            if \"CHRF\" in metrics:\n",
    "                res[\"CHRF\"] = corpus_chrf(hyps, [refs]).score\n",
    "            rows.append(res)\n",
    "        self._model_metrics = pd.DataFrame(rows)\n",
    "\n",
    "    def _record_response_times(self, prediction_cols: List[str], run_id: int) -> None:\n",
    "        # Compute average per model and append to the global table\n",
    "        rows = []\n",
    "        for m in prediction_cols:\n",
    "            times = self._detailed_results[f\"{m} Response Time\"].tolist()\n",
    "            avg = sum(times) / len(times)\n",
    "            rows.append({\"Model\": m, \"Run\": run_id, \"Average Response Time\": avg})\n",
    "        df_rt = pd.DataFrame(rows)\n",
    "        self._all_response_times = pd.concat([self._all_response_times, df_rt], ignore_index=True)\n",
    "\n",
    "    def _generate_report(self) -> None:\n",
    "        with pd.ExcelWriter(\"translation_results.xlsx\", engine='xlsxwriter') as writer:\n",
    "            # Detailed Results\n",
    "            self._detailed_results.to_excel(writer, sheet_name=\"Detailed Results\", index=False)\n",
    "            # Model Metrics\n",
    "            self._model_metrics.to_excel(writer, sheet_name=\"Model Metrics\", index=False)\n",
    "            # Response Times\n",
    "            self._all_response_times.to_excel(writer, sheet_name=\"Response Times\", index=False)\n",
    "\n",
    "            # Formatting for Model Metrics\n",
    "            workbook  = writer.book\n",
    "            ws_metrics = writer.sheets[\"Model Metrics\"]\n",
    "            (r, c) = self._model_metrics.shape\n",
    "            cols = [{'header': h} for h in self._model_metrics.columns]\n",
    "            ws_metrics.add_table(0, 0, r, c-1, {\n",
    "                'columns': cols,\n",
    "                'style': 'Table Style Medium 9'\n",
    "            })\n",
    "            for i, h in enumerate(self._model_metrics.columns):\n",
    "                width = max(self._model_metrics[h].astype(str).map(len).max(), len(h)) + 2\n",
    "                ws_metrics.set_column(i, i, width)\n",
    "\n",
    "            # Formatting for Response Times\n",
    "            ws_rt = writer.sheets[\"Response Times\"]\n",
    "            (r2, c2) = self._all_response_times.shape\n",
    "            cols_rt = [{'header': h} for h in self._all_response_times.columns]\n",
    "            ws_rt.add_table(0, 0, r2, c2-1, {\n",
    "                'columns': cols_rt,\n",
    "                'style': 'Table Style Medium 9'\n",
    "            })\n",
    "            for i, h in enumerate(self._all_response_times.columns):\n",
    "                width = max(self._all_response_times[h].astype(str).map(len).max(), len(h)) + 2\n",
    "                ws_rt.set_column(i, i, width)\n",
    "\n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        return self._detailed_results.copy()\n",
    "\n",
    "    def get_model_metrics(self) -> pd.DataFrame:\n",
    "        return self._model_metrics.copy()\n",
    "\n",
    "    def get_response_times(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the average response-time table for all runs\"\"\"\n",
    "        return self._all_response_times.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'object' has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-80f3e5726ecf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Now you can use it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTranslationEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Successfully imported TranslationEvaluator!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\translation_evaluator.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Accumulate across runs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_response_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Run\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Average Response Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    410\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[0mnan_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_1d_arraylike_from_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnan_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m             \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[1;34m(value, length, dtype)\u001b[0m\n\u001b[0;32m   1219\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1221\u001b[1;33m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'object' has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "from translation_evaluator import TranslationEvaluator\n",
    "\n",
    "# Now you can use it\n",
    "evaluator = TranslationEvaluator()\n",
    "print(\"Successfully imported TranslationEvaluator!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting translation_evaluator\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 7 of C:\\Users\\Hi\\Anaconda3\\lib\\site-packages\\pywin32.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\Hi\\Anaconda3\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "  ModuleNotFoundError: No module named 'pywin32_bootstrap'\n",
      "\n",
      "Remainder of file ignored\n",
      "  ERROR: Could not find a version that satisfies the requirement translation_evaluator (from versions: none)\n",
      "ERROR: No matching distribution found for translation_evaluator\n"
     ]
    }
   ],
   "source": [
    "#pip install translation_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Library imported successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from translation_evaluator import TranslationEvaluator\n",
    "    evaluator = TranslationEvaluator()\n",
    "    print(\"✅ Library imported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Metrics:\n",
      "                             Model      BLEU       CHRF\n",
      "0       base_mad1ad400_translation  6.376716  17.207060\n",
      "1  finetuned_mad1ad400_translation  0.000000  16.187283\n",
      "2   finetuned_helsinki_translation  0.000000  12.292971\n"
     ]
    }
   ],
   "source": [
    "from translation_evaluator import TranslationEvaluator\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = TranslationEvaluator()\n",
    "\n",
    "# Load data (replace with your file path)\n",
    "evaluator.load_data(\"C:/Users/Hi/inputdata.xlsx\")  # or .xlsx\n",
    "\n",
    "# Run evaluation with your configuration\n",
    "evaluator.evaluate(\n",
    "    prediction_cols=[\"base_mad1ad400_translation\", \"finetuned_mad1ad400_translation\", \"finetuned_helsinki_translation\"],\n",
    "    reference_col=\"en\",\n",
    "    metrics=[\"BLEU\", \"CHRF\"],\n",
    "    keep_cols=[\"es\"]\n",
    ")\n",
    "\n",
    "# Optional: Access results programmatically\n",
    "print(\"Model Metrics:\")\n",
    "print(evaluator.get_model_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting translation_evaluator.py\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "from pydantic import BaseModel\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "import traceback\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Roberta large BERTScore API\",\n",
    "    description=\"API for calculating BERTScore between reference and candidate texts\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# we still point TRANSFORMERS_CACHE / HF_HOME at your local MODEL_PATH if needed,\n",
    "# but for the metric itself we pass only the ID string.\n",
    "MODEL_PATH = os.getenv(\"BERT\", \"/appdata/cortex/dev4/shared/libs/huggingface/roberta-large\")\n",
    "os.environ[\"BERT\"] = MODEL_PATH\n",
    "DEVICE = \"cuda\" if os.environ.get(\"CUDA_VISIBLE_DEVICES\") else \"cpu\"\n",
    "\n",
    "# load HF's evaluate metric\n",
    "try:\n",
    "    bscore = evaluate.load(\"bertscore\")\n",
    "    print(\"✅ Loaded HuggingFace BERTScore metric\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to load BERTScore metric:\", e)\n",
    "    bscore = None\n",
    "\n",
    "class ScoreRequest(BaseModel):\n",
    "    reference: list[str]\n",
    "    candidate: list[str]\n",
    "\n",
    "class ScoreResponse(BaseModel):\n",
    "    precision: list[float]\n",
    "    recall:    list[float]\n",
    "    f1:        list[float]\n",
    "    model_type: str = \"roberta-large\"\n",
    "    version:    str = \"0.3.12\"\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\":   \"healthy\" if bscore else \"failed\",\n",
    "        \"model\":    \"HuggingFace BERTScore\",\n",
    "        \"location\": MODEL_PATH,\n",
    "        \"device\":   DEVICE\n",
    "    }\n",
    "\n",
    "@app.post(\"/bertscore/\", response_model=ScoreResponse)\n",
    "async def calculate_score(request: ScoreRequest):\n",
    "    if bscore is None:\n",
    "        raise HTTPException(500, \"Metric failed to load\")\n",
    "    if len(request.reference) != len(request.candidate):\n",
    "        raise HTTPException(400, \"`reference` and `candidate` lists must be same length\")\n",
    "\n",
    "    try:\n",
    "        results = bscore.compute(\n",
    "            predictions=request.candidate,\n",
    "            references=request.reference,\n",
    "            model_type=\"roberta-large\",  # just the ID\n",
    "            lang=\"en\",\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        P, R, F1 = results[\"precision\"], results[\"recall\"], results[\"f1\"]\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        raise HTTPException(500, str(e))\n",
    "\n",
    "    return {\n",
    "        \"precision\": [round(x, 6) for x in P],\n",
    "        \"recall\":    [round(x, 6) for x in R],\n",
    "        \"f1\":        [round(x, 6) for x in F1],\n",
    "        \"model_type\": \"roberta-large\",\n",
    "        \"version\":    \"0.3.12\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/batch-local/\")\n",
    "async def batch_local():\n",
    "    if bscore is None:\n",
    "        raise HTTPException(500, \"Metric failed to load\")\n",
    "\n",
    "    inp = \"/appdata/cortex/dev4/shobha/input_data.xlsx\"\n",
    "    out = \"/appdata/cortex/dev4/shobha/output_data.xlsx\"\n",
    "\n",
    "    if not os.path.exists(inp):\n",
    "        raise HTTPException(404, f\"Input not found: {inp}\")\n",
    "    try:\n",
    "        df = pd.read_excel(inp)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(500, f\"Error reading input: {e}\")\n",
    "    if \"reference\" not in df.columns or \"candidate\" not in df.columns:\n",
    "        raise HTTPException(400, \"Excel must have 'reference' and 'candidate' columns\")\n",
    "\n",
    "    try:\n",
    "        results = bscore.compute(\n",
    "            predictions=df[\"candidate\"].astype(str).tolist(),\n",
    "            references=df[\"reference\"].astype(str).tolist(),\n",
    "            model_type=\"roberta-large\",\n",
    "            lang=\"en\",\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        P, R, F1 = results[\"precision\"], results[\"recall\"], results[\"f1\"]\n",
    "    except Exception as e:\n",
    "        raise HTTPException(500, f\"Scoring error: {e}\")\n",
    "\n",
    "    df[\"precision\"] = [round(x, 6) for x in P]\n",
    "    df[\"recall\"]    = [round(x, 6) for x in R]\n",
    "    df[\"f1\"]        = [round(x, 6) for x in F1]\n",
    "\n",
    "    try:\n",
    "        df.to_excel(out, index=False)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(500, f\"Cannot write Excel: {e}\")\n",
    "\n",
    "    return FileResponse(\n",
    "        out,\n",
    "        filename=os.path.basename(out),\n",
    "        media_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "    )\n",
    "\n",
    "@app.post(\"/batch-bert/\")\n",
    "async def batch_bert(file: UploadFile = File()):\n",
    "    if bscore is None:\n",
    "        raise HTTPException(500, \"Metric failed to load\")\n",
    "    if not file.filename.lower().endswith((\".xls\", \".xlsx\")):\n",
    "        raise HTTPException(400, \"Upload an .xls or .xlsx file\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(file.file)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(400, f\"Cannot read Excel: {e}\")\n",
    "    if \"reference\" not in df.columns or \"candidate\" not in df.columns:\n",
    "        raise HTTPException(400, \"Excel must have 'reference' and 'candidate' columns\")\n",
    "\n",
    "    try:\n",
    "        results = bscore.compute(\n",
    "            predictions=df[\"candidate\"].astype(str).tolist(),\n",
    "            references=df[\"reference\"].astype(str).tolist(),\n",
    "            model_type=\"roberta-large\",\n",
    "            lang=\"en\",\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        P, R, F1 = results[\"precision\"], results[\"recall\"], results[\"f1\"]\n",
    "    except Exception as e:\n",
    "        raise HTTPException(500, f\"Scoring error: {e}\")\n",
    "\n",
    "    df[\"precision\"] = [round(x, 6) for x in P]\n",
    "    df[\"recall\"]    = [round(x, 6) for x in R]\n",
    "    df[\"f1\"]        = [round(x, 6) for x in F1]\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile(suffix=\".xlsx\", delete=False)\n",
    "    df.to_excel(tmp.name, index=False)\n",
    "    tmp.close()\n",
    "    return FileResponse(\n",
    "        tmp.name,\n",
    "        filename=\"bert_results.xlsx\",\n",
    "        media_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_evaluator.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, corpus_bleu, corpus_chrf\n",
    "from typing import List, Optional, Union\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    A translation evaluation system that handles:\n",
    "    - Loading data\n",
    "    - Computing BLEU/CHRF per row\n",
    "    - Tracking response time per row per run\n",
    "    - Aggregating model metrics and average response times\n",
    "    - Exporting to Excel with three sheets:\n",
    "      'Detailed Results', 'Model Metrics', 'Response Times'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        warnings.simplefilter('ignore')\n",
    "        self._data = None\n",
    "        self._detailed_results = None\n",
    "        self._model_metrics = None\n",
    "        # Accumulate across runs\n",
    "        self._all_response_times = pd.DataFrame([], columns=[\"Model\", \"Run\", \"Average Response Time\"])\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        file_path = Path(file_path)\n",
    "        if file_path.suffix == '.csv':\n",
    "            self._data = pd.read_csv(file_path)\n",
    "        elif file_path.suffix in ('.xlsx', '.xls'):\n",
    "            self._data = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col: str = \"English\",\n",
    "        metrics: List[str] = [\"BLEU\", \"CHRF\"],\n",
    "        keep_cols: Optional[List[str]] = None,\n",
    "        run_id: int = 1\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run a full evaluation pass.\n",
    "        - prediction_cols: names of model output columns\n",
    "        - reference_col: name of the reference column\n",
    "        - metrics: which metrics to compute\n",
    "        - keep_cols: extra columns to carry through to detailed results\n",
    "        - run_id: an integer label for this run (e.g. 1 or 2)\n",
    "        \"\"\"\n",
    "        if self._data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        keep_cols = keep_cols or []\n",
    "        self._validate_columns(reference_col, prediction_cols, keep_cols)\n",
    "\n",
    "        # 1. Detailed per-row metrics + response times for this run\n",
    "        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep_cols, run_id)\n",
    "        # 2. Aggregate BLEU/CHRF\n",
    "        self._compute_model_metrics(reference_col, prediction_cols, metrics)\n",
    "        # 3. Record avg response times for this run\n",
    "        self._record_response_times(prediction_cols, run_id)\n",
    "        # 4. Only after run 2, write the full report\n",
    "        if run_id == 2:\n",
    "            self._generate_report()\n",
    "\n",
    "    def _validate_columns(self, reference_col, prediction_cols, keep_cols):\n",
    "        missing = [col for col in [reference_col] + prediction_cols + keep_cols\n",
    "                   if col not in self._data.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns not found in data: {missing}\")\n",
    "\n",
    "    def _compute_detailed_metrics(\n",
    "        self, reference_col: str, prediction_cols: List[str],\n",
    "        metrics: List[str], keep_cols: List[str], run_id: int\n",
    "    ) -> None:\n",
    "        df = self._data.copy()\n",
    "        run_suffix = f\" Run {run_id}\"\n",
    "        response_time_cols = {m: [] for m in prediction_cols}\n",
    "\n",
    "        for m in prediction_cols:\n",
    "            # initialize metric columns on first run\n",
    "            if run_id == 1:\n",
    "                if \"BLEU\" in metrics:\n",
    "                    df[f\"{m} BLEU\"] = None\n",
    "                if \"CHRF\" in metrics:\n",
    "                    df[f\"{m} CHRF\"] = None\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                start = time.time()\n",
    "                if \"BLEU\" in metrics:\n",
    "                    bleu_score = sentence_bleu(row[m], [row[reference_col]]).score\n",
    "                    if run_id == 1:\n",
    "                        df.at[idx, f\"{m} BLEU\"] = bleu_score\n",
    "                if \"CHRF\" in metrics:\n",
    "                    chrf_score = sentence_chrf(row[m], [row[reference_col]]).score\n",
    "                    if run_id == 1:\n",
    "                        df.at[idx, f\"{m} CHRF\"] = chrf_score\n",
    "                end = time.time()\n",
    "                response_time_cols[m].append(end - start)\n",
    "\n",
    "            df[f\"{m} Response Time{run_suffix}\"] = response_time_cols[m]\n",
    "\n",
    "        # build column order\n",
    "        if run_id == 1:\n",
    "            columns = []\n",
    "            for c in keep_cols:\n",
    "                columns.append(c)\n",
    "            for m in prediction_cols:\n",
    "                columns += [m]\n",
    "                if \"BLEU\" in metrics:\n",
    "                    columns.append(f\"{m} BLEU\")\n",
    "                if \"CHRF\" in metrics:\n",
    "                    columns.append(f\"{m} CHRF\")\n",
    "                columns.append(f\"{m} Response Time Run 1\")\n",
    "        else:\n",
    "            columns = self._detailed_results.columns.tolist() + \\\n",
    "                      [f\"{m} Response Time Run 2\" for m in prediction_cols]\n",
    "\n",
    "        self._detailed_results = df[columns]\n",
    "\n",
    "    def _compute_model_metrics(\n",
    "        self, reference_col: str, prediction_cols: List[str],\n",
    "        metrics: List[str]\n",
    "    ) -> None:\n",
    "        rows = []\n",
    "        for m in prediction_cols:\n",
    "            refs = self._data[reference_col].tolist()\n",
    "            hyps = self._data[m].tolist()\n",
    "            res = {\"Model\": m}\n",
    "            if \"BLEU\" in metrics:\n",
    "                res[\"BLEU\"] = corpus_bleu(hyps, [refs]).score\n",
    "            if \"CHRF\" in metrics:\n",
    "                res[\"CHRF\"] = corpus_chrf(hyps, [refs]).score\n",
    "            rows.append(res)\n",
    "        self._model_metrics = pd.DataFrame(rows)\n",
    "\n",
    "    def _record_response_times(self, prediction_cols: List[str], run_id: int) -> None:\n",
    "        rows = []\n",
    "        for m in prediction_cols:\n",
    "            col = f\"{m} Response Time Run {run_id}\"\n",
    "            times = self._detailed_results[col].tolist()\n",
    "            avg = sum(times) / len(times)\n",
    "            rows.append({\"Model\": m, \"Run\": run_id, \"Average Response Time\": avg})\n",
    "        df_rt = pd.DataFrame(rows)\n",
    "        self._all_response_times = pd.concat([self._all_response_times, df_rt], ignore_index=True)\n",
    "\n",
    "    def _generate_report(self) -> None:\n",
    "        with pd.ExcelWriter(\"translation_results.xlsx\", engine='xlsxwriter') as writer:\n",
    "            # Detailed Results\n",
    "            self._detailed_results.to_excel(writer, sheet_name=\"Detailed Results\", index=False)\n",
    "            # Model Metrics\n",
    "            self._model_metrics.to_excel(writer, sheet_name=\"Model Metrics\", index=False)\n",
    "            # Response Times\n",
    "            self._all_response_times.to_excel(writer, sheet_name=\"Response Times\", index=False)\n",
    "\n",
    "            # Formatting for Model Metrics\n",
    "            workbook  = writer.book\n",
    "            ws_metrics = writer.sheets[\"Model Metrics\"]\n",
    "            (r, c) = self._model_metrics.shape\n",
    "            cols = [{'header': h} for h in self._model_metrics.columns]\n",
    "            ws_metrics.add_table(0, 0, r, c-1, {\n",
    "                'columns': cols,\n",
    "                'style': 'Table Style Medium 9'\n",
    "            })\n",
    "            for i, h in enumerate(self._model_metrics.columns):\n",
    "                width = max(self._model_metrics[h].astype(str).map(len).max(), len(h)) + 2\n",
    "                ws_metrics.set_column(i, i, width)\n",
    "\n",
    "            # Formatting for Response Times\n",
    "            ws_rt = writer.sheets[\"Response Times\"]\n",
    "            (r2, c2) = self._all_response_times.shape\n",
    "            cols_rt = [{'header': h} for h in self._all_response_times.columns]\n",
    "            ws_rt.add_table(0, 0, r2, c2-1, {\n",
    "                'columns': cols_rt,\n",
    "                'style': 'Table Style Medium 9'\n",
    "            })\n",
    "            for i, h in enumerate(self._all_response_times.columns):\n",
    "                width = max(self._all_response_times[h].astype(str).map(len).max(), len(h)) + 2\n",
    "                ws_rt.set_column(i, i, width)\n",
    "\n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        return self._detailed_results.copy()\n",
    "\n",
    "    def get_model_metrics(self) -> pd.DataFrame:\n",
    "        return self._model_metrics.copy()\n",
    "\n",
    "    def get_response_times(self) -> pd.DataFrame:\n",
    "        \"\"\"Get the average response-time table for all runs\"\"\"\n",
    "        return self._all_response_times.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation.py\n",
    "\n",
    "from translation_evaluator import TranslationEvaluator\n",
    "\n",
    "def main():\n",
    "    # 1) Instantiate & load your data\n",
    "    evaluator = TranslationEvaluator()\n",
    "    evaluator.load_data(\"your_translations.xlsx\")  # or .csv\n",
    "\n",
    "    # 2) Define which columns to evaluate\n",
    "    models    = [\"model_A\", \"model_B\"]\n",
    "    keep_cols = [\"Sentence ID\"]                   # any extra columns to carry through\n",
    "    ref_col   = \"English\"\n",
    "\n",
    "    # 3) Run twice (run_id=1 and run_id=2)\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        keep_cols=keep_cols,\n",
    "        run_id=1\n",
    "    )\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        keep_cols=keep_cols,\n",
    "        run_id=2\n",
    "    )\n",
    "\n",
    "    # 4) (Optional) Inspect results in the console\n",
    "    print(\"\\n=== Detailed Results ===\")\n",
    "    print(evaluator.get_detailed_results().head(), \"\\n\")\n",
    "\n",
    "    print(\"=== Run Averages ===\")\n",
    "    print(evaluator.get_response_times(), \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score\n",
    "\n",
    "# batch_bertscore.py\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# Path to your locally-saved roberta-large checkpoint\n",
    "MODEL_PATH = os.getenv(\n",
    "    \"BERT\",\n",
    "    \"/appdata/cortex/dev4/shared/libs/huggingface/roberta-large\"\n",
    ")\n",
    "\n",
    "# Use GPU if CUDA_VISIBLE_DEVICES is set, else CPU\n",
    "DEVICE = \"cuda\" if os.getenv(\"CUDA_VISIBLE_DEVICES\") else \"cpu\"\n",
    "\n",
    "# Load the 🤗 Evaluate BERTScore metric (no `from bert_score import …`)\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# FASTAPI SETUP\n",
    "app = FastAPI(\n",
    "    title=\"RoBERTa-Large BERTScore API\",\n",
    "    description=\"Compute BERTScore with your local RoBERTa-large checkpoint\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "class ScoreRequest(BaseModel):\n",
    "    reference: list[str]\n",
    "    candidate: list[str]\n",
    "\n",
    "class ScoreResponse(BaseModel):\n",
    "    precision: list[float]\n",
    "    recall:    list[float]\n",
    "    f1:        list[float]\n",
    "    model_type: str\n",
    "    version:    str\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model\":  f\"roberta-large @ {MODEL_PATH}\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/bertscore/\", response_model=ScoreResponse)\n",
    "async def calculate_score(request: ScoreRequest):\n",
    "    if len(request.reference) != len(request.candidate):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"`reference` and `candidate` must be the same length\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        results = bertscore.compute(\n",
    "            predictions=request.candidate,\n",
    "            references = request.reference,\n",
    "            model_type = MODEL_PATH,\n",
    "            device     = DEVICE\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"scoring error: {e}\")\n",
    "\n",
    "    return ScoreResponse(\n",
    "        precision=results[\"precision\"],\n",
    "        recall=   results[\"recall\"],\n",
    "        f1=       results[\"f1\"],\n",
    "        model_type=\"roberta-large\",\n",
    "        version=\"1.0\"\n",
    "    )\n",
    "\n",
    "@app.get(\"/batch-local/\")\n",
    "async def batch_local():\n",
    "    inp = \"/appdata/cortex/dev4/shobha/input_data.xlsx\"\n",
    "    if not os.path.exists(inp):\n",
    "        raise HTTPException(status_code=404, detail=f\"input not found: {inp}\")\n",
    "    return FileResponse(\n",
    "        path=inp,\n",
    "        filename=os.path.basename(inp),\n",
    "        media_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "    )\n",
    "\n",
    "@app.post(\"/batch-bertscore/\")\n",
    "async def batch_bertscore(file: UploadFile = File(...)):\n",
    "    # 1) Read the uploaded Excel file\n",
    "    try:\n",
    "        df = pd.read_excel(file.file)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"cannot read Excel: {e}\")\n",
    "\n",
    "    # 2) Validate required columns\n",
    "    if \"reference\" not in df.columns or \"candidate\" not in df.columns:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Excel must have 'reference' and 'candidate' columns\"\n",
    "        )\n",
    "\n",
    "    # 3) Compute BERTScore in batch\n",
    "    try:\n",
    "        results = bertscore.compute(\n",
    "            predictions=df[\"candidate\"].astype(str).tolist(),\n",
    "            references = df[\"reference\"].astype(str).tolist(),\n",
    "            model_type = MODEL_PATH,\n",
    "            device     = DEVICE\n",
    "        )\n",
    "        df[\"precision\"] = results[\"precision\"]\n",
    "        df[\"recall\"]    = results[\"recall\"]\n",
    "        df[\"f1\"]        = results[\"f1\"]\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"scoring error: {e}\")\n",
    "\n",
    "    # 4) Write results to a temporary Excel and return\n",
    "    tmp = tempfile.NamedTemporaryFile(suffix=\".xlsx\", delete=False)\n",
    "    out = tmp.name\n",
    "    tmp.close()\n",
    "\n",
    "    try:\n",
    "        df.to_excel(out, index=False)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"cannot write Excel: {e}\")\n",
    "\n",
    "    return FileResponse(\n",
    "        path=out,\n",
    "        filename=\"roberta_results.xlsx\",\n",
    "        media_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_evaluator.py\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, corpus_bleu, corpus_chrf\n",
    "from typing import List, Optional, Union\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    A translation evaluation system that handles:\n",
    "    - Loading data\n",
    "    - Computing BLEU/CHRF per row\n",
    "    - Tracking response time per row per run\n",
    "    - Aggregating model metrics and average response times\n",
    "    - Exporting to Excel with four sheets:\n",
    "      'Detailed Results', 'Model Metrics', 'Response Times', 'Run Averages'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        warnings.simplefilter('ignore')\n",
    "        self._data = None\n",
    "        self._detailed_results = None\n",
    "        self._model_metrics = None\n",
    "        # Holds per‑run averages across runs\n",
    "        self._all_response_times = pd.DataFrame(\n",
    "            [], columns=[\"Model\", \"Run\", \"Average Response Time\"]\n",
    "        )\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        file_path = Path(file_path)\n",
    "        if file_path.suffix == '.csv':\n",
    "            self._data = pd.read_csv(file_path)\n",
    "        elif file_path.suffix in ('.xlsx', '.xls'):\n",
    "            self._data = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col: str = \"English\",\n",
    "        metrics: List[str] = [\"BLEU\", \"CHRF\"],\n",
    "        keep_cols: Optional[List[str]] = None,\n",
    "        run_id: int = 1\n",
    "    ) -> None:\n",
    "        if self._data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        keep_cols = keep_cols or []\n",
    "        self._validate_columns(reference_col, prediction_cols, keep_cols)\n",
    "\n",
    "        # 1. Detailed per-row metrics + response times\n",
    "        self._compute_detailed_metrics(\n",
    "            reference_col, prediction_cols, metrics, keep_cols, run_id\n",
    "        )\n",
    "        # 2. Aggregate corpus‑level metrics\n",
    "        self._compute_model_metrics(reference_col, prediction_cols, metrics)\n",
    "        # 3. Record per‑run averages\n",
    "        self._record_response_times(prediction_cols, run_id)\n",
    "        # 4. After run 2, write out the full Excel report\n",
    "        if run_id == 2:\n",
    "            self._generate_report()\n",
    "\n",
    "    def _validate_columns(self, reference_col, prediction_cols, keep_cols):\n",
    "        missing = [\n",
    "            col for col in [reference_col] + prediction_cols + keep_cols\n",
    "            if col not in self._data.columns\n",
    "        ]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns not found in data: {missing}\")\n",
    "\n",
    "    def _compute_detailed_metrics(\n",
    "        self, reference_col: str, prediction_cols: List[str],\n",
    "        metrics: List[str], keep_cols: List[str], run_id: int\n",
    "    ) -> None:\n",
    "        df = self._data.copy()\n",
    "        run_suffix = f\" Run {run_id}\"\n",
    "        response_time_cols = {m: [] for m in prediction_cols}\n",
    "\n",
    "        for m in prediction_cols:\n",
    "            # initialize score columns on first run\n",
    "            if run_id == 1:\n",
    "                if \"BLEU\" in metrics:\n",
    "                    df[f\"{m} BLEU\"] = None\n",
    "                if \"CHRF\" in metrics:\n",
    "                    df[f\"{m} CHRF\"] = None\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                start = time.time()\n",
    "                if \"BLEU\" in metrics:\n",
    "                    bleu_score = sentence_bleu(\n",
    "                        row[m], [row[reference_col]]\n",
    "                    ).score\n",
    "                    if run_id == 1:\n",
    "                        df.at[idx, f\"{m} BLEU\"] = bleu_score\n",
    "                if \"CHRF\" in metrics:\n",
    "                    chrf_score = sentence_chrf(\n",
    "                        row[m], [row[reference_col]]\n",
    "                    ).score\n",
    "                    if run_id == 1:\n",
    "                        df.at[idx, f\"{m} CHRF\"] = chrf_score\n",
    "                end = time.time()\n",
    "                response_time_cols[m].append(end - start)\n",
    "\n",
    "            # append this run's timing column\n",
    "            df[f\"{m} Response Time{run_suffix}\"] = response_time_cols[m]\n",
    "\n",
    "        # build or extend column order\n",
    "        if run_id == 1:\n",
    "            cols = []\n",
    "            for c in keep_cols:\n",
    "                cols.append(c)\n",
    "            for m in prediction_cols:\n",
    "                cols.append(m)\n",
    "                if \"BLEU\" in metrics:\n",
    "                    cols.append(f\"{m} BLEU\")\n",
    "                if \"CHRF\" in metrics:\n",
    "                    cols.append(f\"{m} CHRF\")\n",
    "                cols.append(f\"{m} Response Time Run 1\")\n",
    "        else:\n",
    "            cols = self._detailed_results.columns.tolist() + [\n",
    "                f\"{m} Response Time Run 2\" for m in prediction_cols\n",
    "            ]\n",
    "\n",
    "        self._detailed_results = df[cols]\n",
    "\n",
    "    def _compute_model_metrics(\n",
    "        self, reference_col: str, prediction_cols: List[str],\n",
    "        metrics: List[str]\n",
    "    ) -> None:\n",
    "        rows = []\n",
    "        for m in prediction_cols:\n",
    "            refs = self._data[reference_col].tolist()\n",
    "            hyps = self._data[m].tolist()\n",
    "            entry = {\"Model\": m}\n",
    "            if \"BLEU\" in metrics:\n",
    "                entry[\"BLEU\"] = corpus_bleu(hyps, [refs]).score\n",
    "            if \"CHRF\" in metrics:\n",
    "                entry[\"CHRF\"] = corpus_chrf(hyps, [refs]).score\n",
    "            rows.append(entry)\n",
    "        self._model_metrics = pd.DataFrame(rows)\n",
    "\n",
    "    def _record_response_times(\n",
    "        self, prediction_cols: List[str], run_id: int\n",
    "    ) -> None:\n",
    "        rows = []\n",
    "        for m in prediction_cols:\n",
    "            col = f\"{m} Response Time Run {run_id}\"\n",
    "            times = self._detailed_results[col].tolist()\n",
    "            avg = sum(times) / len(times)\n",
    "            rows.append({\n",
    "                \"Model\": m,\n",
    "                \"Run\": run_id,\n",
    "                \"Average Response Time\": avg\n",
    "            })\n",
    "        df_rt = pd.DataFrame(rows)\n",
    "        self._all_response_times = pd.concat(\n",
    "            [self._all_response_times, df_rt], ignore_index=True\n",
    "        )\n",
    "\n",
    "    def _generate_report(self) -> None:\n",
    "        # also build the Run Averages sheet\n",
    "        run_avgs = (\n",
    "            self._all_response_times\n",
    "                .pivot(index=\"Model\", columns=\"Run\", values=\"Average Response Time\")\n",
    "                .reset_index()\n",
    "        )\n",
    "        run_avgs[\"Overall Avg (s)\"] = run_avgs[[1, 2]].mean(axis=1)\n",
    "\n",
    "        with pd.ExcelWriter(\"translation_results.xlsx\", engine='xlsxwriter') as writer:\n",
    "            # 1) Detailed Results\n",
    "            self._detailed_results.to_excel(\n",
    "                writer, sheet_name=\"Detailed Results\", index=False\n",
    "            )\n",
    "            # 2) Corpus‑level Metrics\n",
    "            self._model_metrics.to_excel(\n",
    "                writer, sheet_name=\"Model Metrics\", index=False\n",
    "            )\n",
    "            # 3) Per‑run averages\n",
    "            self._all_response_times.to_excel(\n",
    "                writer, sheet_name=\"Response Times\", index=False\n",
    "            )\n",
    "            # 4) Overall Run Averages\n",
    "            run_avgs.to_excel(writer, sheet_name=\"Run Averages\", index=False)\n",
    "\n",
    "            # (Optional) add table formatting for each sheet...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_evaluation.py\n",
    "\n",
    "from translation_evaluator import TranslationEvaluator\n",
    "\n",
    "def main():\n",
    "    evaluator = TranslationEvaluator()\n",
    "    evaluator.load_data(\"your_translations.xlsx\")  # or .csv\n",
    "\n",
    "    models    = [\"model_A\", \"model_B\"]   # your prediction columns\n",
    "    keep_cols = [\"Sentence ID\"]          # any extra columns to carry through\n",
    "    ref_col   = \"English\"\n",
    "\n",
    "    # Run 1\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        keep_cols=keep_cols,\n",
    "        run_id=1\n",
    "    )\n",
    "    # Run 2\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        keep_cols=keep_cols,\n",
    "        run_id=2\n",
    "    )\n",
    "\n",
    "    # (Optional) print to console\n",
    "    print(\"=== Detailed Results ===\")\n",
    "    print(evaluator.get_detailed_results().head(), \"\\n\")\n",
    "    print(\"=== Run Averages ===\")\n",
    "    print(evaluator.get_response_times(), \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    #pip install pandas sacrebleu xlsxwriter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
