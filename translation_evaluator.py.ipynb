{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation_evaluator.py\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sacrebleu import sentence_bleu, sentence_chrf, sentence_chrf2\n",
    "\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    \"\"\"\n",
    "    - load_data(): CSV or Excel\n",
    "    - evaluate(): computes per-model & per-metric columns (plus optional per-model timing)\n",
    "    - get_detailed_results(): DataFrame with those columns\n",
    "    - get_model_metrics(): dict of all metric means\n",
    "    - get_response_times(): DataFrame of run_id vs avg_response_time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data: pd.DataFrame | None = None\n",
    "        self._detailed_results: pd.DataFrame = pd.DataFrame()\n",
    "        self._model_metrics: dict[str, float] = {}\n",
    "        self._response_times: pd.DataFrame = pd.DataFrame(\n",
    "            [], columns=[\"run_id\", \"avg_response_time\"]\n",
    "        )\n",
    "\n",
    "    def load_data(self, file_path: Union[str, Path]) -> None:\n",
    "        p = Path(file_path)\n",
    "        if p.suffix.lower() == \".csv\":\n",
    "            self._data = pd.read_csv(p)\n",
    "        elif p.suffix.lower() in (\".xls\", \".xlsx\"):\n",
    "            self._data = pd.read_excel(p)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format (must be .csv, .xls or .xlsx)\")\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        prediction_cols: List[str],\n",
    "        reference_col:    str,\n",
    "        metrics:          List[str],\n",
    "        keep_cols:        List[str],\n",
    "        run_id:           int | None = None,\n",
    "        measure_time:     bool      = False,\n",
    "    ) -> None:\n",
    "        if self._data is None:\n",
    "            raise RuntimeError(\"No data loaded; call load_data() first.\")\n",
    "\n",
    "        rows = []\n",
    "        time_records: list[float] = []\n",
    "\n",
    "        for _, r in self._data.iterrows():\n",
    "            rec = {c: r[c] for c in keep_cols}\n",
    "            ref = str(r[reference_col])\n",
    "\n",
    "            for col in prediction_cols:\n",
    "                pred = str(r[col])\n",
    "                if measure_time:\n",
    "                    start = time.time()\n",
    "\n",
    "                # compute each metric and store under \"{col} {METRIC}\"\n",
    "                for m in metrics:\n",
    "                    m_up = m.upper()\n",
    "                    if m_up == \"BLEU\":\n",
    "                        sc = sentence_bleu(pred, [ref]).score\n",
    "                    elif m_up == \"CHRF\":\n",
    "                        sc = sentence_chrf(pred, [ref]).score\n",
    "                    elif m_up in (\"CHRF++\", \"CHRFF++\", \"CHR F++\"):\n",
    "                        sc = sentence_chrf2(pred, [ref]).score\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported metric: {m!r}\")\n",
    "                    rec[f\"{col} {m_up}\"] = sc\n",
    "\n",
    "                # optional timing column per model\n",
    "                if measure_time and run_id is not None:\n",
    "                    elapsed = time.time() - start\n",
    "                    rec[f\"{col} Response Time Run {run_id}\"] = elapsed\n",
    "                    time_records.append(elapsed)\n",
    "\n",
    "            rows.append(rec)\n",
    "\n",
    "        # build detailed-results DataFrame\n",
    "        self._detailed_results = pd.DataFrame(rows)\n",
    "\n",
    "        # aggregate model metrics: mean of every \"{col} {METRIC}\"\n",
    "        metric_cols = [f\"{c} {m.upper()}\" for c in prediction_cols for m in metrics]\n",
    "        self._model_metrics = {\n",
    "            col: float(self._detailed_results[col].mean()) for col in metric_cols\n",
    "        }\n",
    "\n",
    "        # if timing was measured, record the run average\n",
    "        if measure_time and run_id is not None:\n",
    "            avg = float(np.mean(time_records))\n",
    "            new = pd.DataFrame([{\"run_id\": run_id, \"avg_response_time\": avg}])\n",
    "            self._response_times = pd.concat(\n",
    "                [self._response_times, new], ignore_index=True\n",
    "            )\n",
    "\n",
    "    def get_detailed_results(self) -> pd.DataFrame:\n",
    "        return self._detailed_results.copy()\n",
    "\n",
    "    def get_model_metrics(self) -> dict[str, float]:\n",
    "        return dict(self._model_metrics)\n",
    "\n",
    "    def get_response_times(self) -> pd.DataFrame:\n",
    "        return self._response_times.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation_evaluator import TranslationEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    evaluator = TranslationEvaluator()\n",
    "    evaluator.load_data(\"input_data.xlsx\")\n",
    "\n",
    "    models   = ['base_madlad400_translation', 'finetuned_madlad400_translation']\n",
    "    keep     = [\"en\", \"es\"]\n",
    "    ref_col  = \"en\"\n",
    "    metrics  = [\"BLEU\", \"ChrF\", \"ChrF++\"]\n",
    "\n",
    "    # Run #1, with timing\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=1,\n",
    "        measure_time=True\n",
    "    )\n",
    "\n",
    "    # Run #2, without timing\n",
    "    evaluator.evaluate(\n",
    "        prediction_cols=models,\n",
    "        reference_col=ref_col,\n",
    "        metrics=metrics,\n",
    "        keep_cols=keep,\n",
    "        run_id=2,\n",
    "        measure_time=False\n",
    "    )\n",
    "\n",
    "    # View detailed results\n",
    "    df = evaluator.get_detailed_results()\n",
    "    print(\"Columns in detailed results:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Example: select exactly the columns you need:\n",
    "    want = [\n",
    "      'base_madlad400_translation BLEU',\n",
    "      'base_madlad400_translation CHRF',\n",
    "      'base_madlad400_translation Response Time Run 1',\n",
    "      # etc. for the rest...\n",
    "    ]\n",
    "    print(df[want].head(), \"\\n\")\n",
    "\n",
    "    # View aggregated metrics\n",
    "    print(\"Aggregated metrics:\")\n",
    "    print(pd.DataFrame([evaluator.get_model_metrics()]), \"\\n\")\n",
    "\n",
    "    # View timing table\n",
    "    print(\"Response times by run:\")\n",
    "    print(evaluator.get_response_times())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
