# -*- coding: utf-8 -*-
"""
Translation_evaluator_Research.py

A class for:
  1) loading translation / reference data
  2) computing per-sentence (detailed) and corpus-level (summary) metrics
     + BLEU, ChrF, ChrF++ (charf with word_order=2)
  3) optionally measuring per-sentence latency
  4) writing three sheets (Detailed, Summary, Timings) to an Excel workbook
"""

import time
from pathlib import Path
from typing import List, Union, Optional
import pandas as pd
from sacrebleu import sentence_bleu, sentence_chrf
from sacrebleu.metrics import CHRF
from sacrebleu import corpus_bleu, corpus_chrf


class TranslationEvaluator:
    def __init__(self, output_file: Union[str, Path] = "output_data.xlsx"):
        self.output_file = Path(output_file)
        self._data: pd.DataFrame    = pd.DataFrame()
        self._detailed: pd.DataFrame= pd.DataFrame()
        self._summary: pd.DataFrame = pd.DataFrame()
        self._timings: pd.DataFrame = pd.DataFrame()
        # for corpus-level ChrF++
        self._corpus_chrf2 = CHRF(word_order=2)

    def load_data(self, file: Union[str, Path], sheet_name: Union[int,str]=0) -> None:
        p = Path(file)
        if p.suffix.lower() in [".xls", ".xlsx"]:
            self._data = pd.read_excel(p, sheet_name=sheet_name)
        elif p.suffix.lower() == ".csv":
            self._data = pd.read_csv(p)
        else:
            raise ValueError(f"Unsupported file type: {p.suffix!r}")
        # clear any old results
        self._detailed = pd.DataFrame()
        self._summary  = pd.DataFrame()
        self._timings  = pd.DataFrame()

    def evaluate(
        self,
        *,
        prediction_cols: List[str],
        reference_col: str = "en",
        metrics: List[str] = ["BLEU", "ChrF", "ChrF++"],
        keep_cols: Optional[List[str]] = None,
        run_id: int = 1,
        measure_time: bool = False,
    ) -> None:
        df = self._data
        keeps = keep_cols or []
        want = set(prediction_cols) | {reference_col} | set(keeps)
        missing = want - set(df.columns)
        if missing:
            raise ValueError(f"Missing columns in data: {missing}")

        # 1) DETAILED
        rows = []
        for _, row in df.iterrows():
            ref = str(row[reference_col])
            for model in prediction_cols:
                pred = str(row[model])
                rec = {c: row[c] for c in keeps}
                rec["model"] = model
                rec["run"]   = run_id
                rec[reference_col] = ref

                if "BLEU" in metrics:
                    rec["BLEU"] = sentence_bleu(pred, [ref]).score
                if "ChrF" in metrics:
                    rec["ChrF"] = sentence_chrf(pred, [ref]).score
                if "ChrF++" in metrics:
                    rec["ChrF++"] = CHRF(word_order=2).score(pred, [ref])

                if measure_time:
                    t0 = time.perf_counter()
                    _ = sentence_bleu(pred, [ref])
                    rec["response_time"] = time.perf_counter() - t0

                rows.append(rec)

        det = pd.DataFrame(rows)
        self._detailed = pd.concat([self._detailed, det], ignore_index=True)

        # 2) SUMMARY (corpus-level)
        sum_rows = []
        for model in prediction_cols:
            rec = {"model": model, "run": run_id}
            preds = df[model].astype(str).tolist()
            refs  = df[reference_col].astype(str).tolist()
            if "BLEU" in metrics:
                rec["BLEU"] = corpus_bleu(preds, [refs]).score
            if "ChrF" in metrics:
                rec["ChrF"] = corpus_chrf(preds, [refs]).score
            if "ChrF++" in metrics:
                rec["ChrF++"] = self._corpus_chrf2.corpus_score(preds, [refs]).score
            sum_rows.append(rec)

        self._summary = pd.concat(
            [self._summary, pd.DataFrame(sum_rows)], ignore_index=True
        )

        # 3) TIMINGS (average response_time per model/run)
        if measure_time:
            avg = det.groupby(["model", "run"], as_index=False)[
                "response_time"
            ].mean()
            self._timings = pd.concat([self._timings, avg], ignore_index=True




    def save_excel(self) -> None:
        if self._detailed.empty:
            raise RuntimeError("No data – call .evaluate() first.")

        with pd.ExcelWriter(self.output_file, engine="openpyxl") as w:
            # ─── DETAILED ────────────────────────────────────────────────────
            det = self._detailed

            # keep everything _except_ the metric columns as our index
            idx_cols = [c for c in det.columns if c not in ("BLEU","ChrF","ChrF++","response_time")]
            # metric columns are whatever's left
            val_cols = [c for c in det.columns if c not in idx_cols]

            det_pivot = det.pivot_table(
                index=idx_cols,
                columns=["run","model"],      # <-- swap run/model here
                values=val_cols,
                aggfunc="first",
            )
            det_pivot.to_excel(w, sheet_name="Detailed")

            # ─── SUMMARY ─────────────────────────────────────────────────────
            sm = self._summary
            # first melt into long form
            long = (
                sm
                .melt(id_vars=["model","run"], var_name="metric", value_name="score")
            )
            # then pivot so metrics down the left, runs & models across the top
            sum_pivot = long.pivot(
                index="metric",
                columns=["run","model"],
                values="score",
            )
            sum_pivot.to_excel(w, sheet_name="Summary")

            # ─── TIMINGS ────────────────────────────────────────────────────
            if not self._timings.empty:
                tm = self._timings
                # pivot so each run is its own column
                tim_pivot = tm.pivot(
                    index="model",
                    columns="run",
                    values="response_time"
                )
                tim_pivot.to_excel(w, sheet_name="Timings")







def save_excel(self):
    det = self._detailed
    if det is None or det.empty:
        raise RuntimeError("No detailed results! Call .evaluate() twice first.")

    # 1) Detailed sheet
    # choose your keep‐cols (here: reference + any extras)
    keep = [self.reference_col] + (self.keep_cols or [])
    # metrics are everything else except model/run/keep
    metrics = [c for c in det.columns if c not in keep + ["model","run"]]

    det_pivot = det.pivot_table(
        index=keep,
        columns=["model","run"],
        values=metrics,
        aggfunc="first",
    )

    # 2) Summary sheet (as before)
    sm = self._summary
    long = sm.melt(
        id_vars=["model","run"],
        var_name="metric",
        value_name="score"
    )
    sum_pivot = long.pivot(
        index="metric",
        columns=["run","model"],
        values="score"
    )

    # 3) Timing sheet
    tm = self._timings
    tim_pivot = tm.pivot(
        index="model",
        columns="run",
        values="response_time"
    )

    # 4) Write all three
    with pd.ExcelWriter(self._output_file, engine="openpyxl") as w:
        det_pivot.to_excel(w, sheet_name="Detailed")
        sum_pivot.to_excel(w, sheet_name="Summary")
        tim_pivot.to_excel(w, sheet_name="Timings")